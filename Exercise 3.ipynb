{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "data = np.loadtxt('smoking.txt')\n",
    "age = data[:,0]\n",
    "FEV1 = data[:,1]\n",
    "height = data[:,2]\n",
    "gender = data[:,3]\n",
    "smoking_status = data[:,4]\n",
    "weight = data[:,5]\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)initiate iteration weight at 0 or normal distribtuion of (mean=0, var = 1) <br>\n",
    "2)for t = 0 to max <br>\n",
    "3) within loop calculate gradient: -1/N sum of y_n*x_n/ (1+ exp(y_n*w.transpose*x_n)) <br>\n",
    "gradient has dimensions of a vector<br>\n",
    "4) v_t = -gradient\n",
    "\n",
    "multiply x_n vector with the weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 1) x: the independent variable, as a N dimensional vector as a numpy array\n",
    "#        2) y: the dependent variable, as a N dimensional vector as a numpy array\n",
    "#\n",
    "# output: 1) the alpha parameter\n",
    "#         2) the beta parameter\n",
    "def univarlinreg(x,y):\n",
    "    #following part is from textbook:\n",
    "    #defining functions for calculations\n",
    "    def dot(v, w):\n",
    "            \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "            return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "    def sum_of_squares(v):\n",
    "        \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "        return dot(v, v)\n",
    "    def mean(x):\n",
    "        return sum(x) / len(x)\n",
    "    def de_mean(x):\n",
    "        \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "        x_bar = mean(x)\n",
    "        return [x_i - x_bar for x_i in x]\n",
    "    def variance(x):\n",
    "        \"\"\"assumes x has at least two elements\"\"\"\n",
    "        n = len(x)\n",
    "        deviations = de_mean(x)\n",
    "        return sum_of_squares(deviations) / (n - 1)\n",
    "    def std_dv(x):\n",
    "        return math.sqrt(variance(x))\n",
    "    def covariance(x,y):\n",
    "        n = len(x)\n",
    "        return dot(de_mean(x), de_mean(y))/(n-1)\n",
    "        \n",
    "    def corr(x,y):\n",
    "        stdev_x = std_dv(x)\n",
    "        stdev_y = std_dv(y)\n",
    "        if stdev_x > 0 and stdev_y > 0:\n",
    "            return covariance(x,y)/stdev_x / stdev_y\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #univariate linear regression part\n",
    "    beta = corr(x, y) * std_dv(y)/ std_dv(x)\n",
    "    alpha = mean(y) - beta*mean(x)\n",
    "    \n",
    "    return alpha, beta\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.431648105733\n",
      "0.222040975958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x108cd2290>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEZCAYAAAB2AoVaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXl8VOXV+L8nmQQSSCAJu+zBpSoqFq2/2lfSCsRaS1W6\naEVBfaXUXXCjYMUq0laxdlHrBuJaa6kKVglYDYpVK69Yd+sCuLDvO1nm/P64d5KZzARyb+5kJpnz\n/XyeDzPPvffcc2+Gc597nvOcI6qKYRiG0bbJSrUChmEYRvIxY28YhpEBmLE3DMPIAMzYG4ZhZABm\n7A3DMDIAM/aGYRgZgBl7Y5+ISKWInN9C5/q5iKwVkW0iUtQS5zSMTMGMvYGIrBCRXSKyXUTWiMhs\nEengbla37U9GfxEJi4iv35SI5AAzgRNVtVBVNzcif3tUW+ZuGycitQ22bRORniKyQERuSHC+H4jI\nahHJEpFvi8iLIrJFRJb70d8w0h0z9gY4xvwUVS0AjgaGAlN9yhKfx/UA2gMf7Ge/Tqpa4LYhUf2v\nRPUXuA+M1cADwJgEcs4GHlbVMLADuA+4yqfuvvD7YDQMP9iPzYhBVVcBC4DDGm4Th6num8BaEZkj\nIoXu5pfcf7e4I+tvJDi+nYjcLiJfue13IpIrIgdRb+S3iMjzPlRv7CHzNFAiIv8TpUcR8D3gQfea\n31DVR4D9jupF5DkRuahB339E5FT38yEiskhENorIhyLyo6j9HhCRu0TkWRHZAZSJyMki8r77JvKl\niExy9x0nIi83OE9YRAa6n08WkfcaHpdA31IReUFENojIehF5WEQ6RW0/WkSWuXL+KiKPi8iNUdtP\nEZG3RGSziLwiIoP3d4+M9MSMvRFBAESkD/BdYFmCfc4FxgJlwECgI/And1vEmEZG3q8nOH4KcCxw\npNuOBaaq6n+pf7h0UtXh+9OzqajqbuCvwDlR3T8GPlDVd7zIcnkUOLNOGZFDgb7AP1zX1yLgYaAr\ncAZwp4h8Ler4M4EbVbUj8C/gfuACVS3EuQcvNFGP+4HxTTxuOtAT+BrQB5jm6p4LPAnMAoqAx4BT\ncd12IjIkoh9QDNwNzHOPM1oZZuwNcAzoUyKyGXgZqARuTrDfWcBMVV2hqjuBycAZrjuiKUb4p8Cv\nVHWDqm4AbsBxp0R0aAob3FHmZhGZGNV/XFT/ZhH5OGrbHOCHUUbqHLfPD08BR7kPRXDuyVxVrQZO\nAZar6hxVDavqW8DfgR9FH6+qrwKo6h6gCjhMRApVdauqJnrIJqJJx6nqp6r6T1Wtdu/574Bh7ubj\ngGxV/aOq1qrqk8C/ow4fD9ztvvmoqj4I7HWPM1oZZuwNcEZyP1DVIlXtr6oXq+reBPv1BFZGff8c\nCAHdm3ieXgmO7+VR1xJXzyJVvS2q/7Wo/iJVPTCyQVVfATYAp4lIKXAMzgjdM6q6HfgH9aP7M4BH\n3M/9gG9EP3RwHnCR+6PAFw1EjgZOBlaIE/nUVEPapONEpLuI/MV19WwFHgJK3M29gK8aHBKtXz9g\nUoPr6Y3zOzBaGWbsDS+sAvpHfe8L1ABraULETiPHrwpIt/3xIM6IfgywQFXXN0PWY8CZIvL/gPaq\n+qLb/zmwuMFDp0BVL2pMkKouVdVTcdw+T+G4nAB2AvmR/USkRxOPa8jNQC1wuKp2wnmTivy/Xw0c\n0GD/vlGfPwemN7iejqr6eGPXY6QvZuwNLzwGXOGGQXbEMSR/cSNa1gNhoHQ/x08VkS4i0gX4Jc5I\nsyV4EBgB/C8NXDjuxHN7IMf92m4/fulncUa9NwB/iep/BjhIRMaISI7bjhGRQyKnanDeHBE5S0Q6\nqWotsB3HMAP8B8dNc6Sr27QmHteQjjgPjm0icgCxEUevArUicrGIhETkBzhvPRHuBSaIyLHuPeog\nIt9z//ZGK8OMveGFWTjG+SXgM2AXcAmAqu7CmQh8xX3lPzbB8TcBS4G33bbU7Yuwv7eDxrYr8P8k\nNs5+u4h8vW4H1ZXAKzij5XkNjh/mXss/cCYwd+NEJCU+mWoVji/+RKLcQaq6AxiJ49r5CmfkPAOI\nPDgSrVkYAyx3XSzjceYAcCetfwU8D3yEM5ei+zsuATfghNNuBeYDcyNy3Os4HTgf2OzKeAZnPgBV\n/T+cydk/AZuAj4md6DZaEZLM4iUicjCxI5+BwHWq+oekndQwDN+IyOvAnarqdwLbSFOSauxjTuRE\nbHwFHKuqDSepDMNIASJyAvBfnAnss4A7gYGqujalihmBE2rBcw0HPjVDbxhpxcE4k7sdgE+BH5qh\nb5u05Mh+FrBUVe9skRMahmEYdbSIsXcjG74CDm1myJthGIbhg5Zy43wX+L+Ghl5EWua1wjAMo42h\nqp5Sh7RU6OWZODHWcahq2rXrr78+5TqYTqZTJuplOjWt+SHpxt5NDjUcJy7ZMAzDSAFJd+OokzCr\nS7LPYxiGYTSOraBNQFlZWapViMN0ahqmU9NJR71Mp+TRYqGXCU8uoqk8v2EYRmtERNA0naA1DMMw\nUogZe8MwjAzAjL1hGEYGYMbeMAwjAzBjbxiGkQGYsTcMw8gAzNgbhmFkAGbsDcMwMgAz9oZhGBmA\nGXvDMIwMwIy9YRhGBmDG3jAMIwMwY28YhpEBmLE3DMPIAMzYG4ZhZABm7A3DMDIAM/aGYRgZgBl7\nwzCMDMCMvWEYRgZgxt4wDCMDMGNvGIaRASTV2ItIZxH5m4h8ICLvi8hxyTyfYRiGkZhQkuX/HnhW\nVX8oIiGgQ5LPZxiGYSRAVDU5gkU6ActUdeA+9tFknd8wDKOtIiKoqng5JplunAHAehGZLSJvisi9\nIpKfxPMZhmEYjZBMN04IOBq4WFXfEJHbgWuBX0bvNG3atLrPZWVllJWVJVElwzCM1kdlZSWVlZXN\nkpFMN04P4FVVHeB+/xZwraqeErWPuXEMwzA8klZuHFVdA3whIge5XcOB95J1PsMwDKNxkjayBxCR\nI4H7gFzgU+BcVd0atd1G9oZhGB7xM7JPqrHf78nN2BtGq6WiooKZM+8BYNKk8ZSXl6dYo8zBjL1h\nGC1CRUUFp502lt27fwNAXt41PPnkHDP4LYQZe8MwWoSRI0ezaNEoYKzbM4cRI+axcOHcVKqVMaTV\nBK1hGIaRPiQ7XYJhGG2QSZPGs2TJWHbvdr7n5V3DpElzUquUsU9sZG8YhmfKy8t58knHdTNixLy0\n8ddPnz6dkpJBlJQMYvr06alWJ60wn71hGG2C6dOnM3Xqb4E/uD2XctNNVzNlypRUqpUUbILWMIyM\npaRkEJs2XUf0pHFx8Y1s3PhJKtVKCjZBaxiGYSTEjL1hGG2CiRPPBS4F5rjtUrfPAHPjGIbRhpg+\nfTq33TYbcIx/W/TXg/nsDcMwMgLz2RuGYRgJMWNvGIaRAZixNwzDyADM2BuGYWQAZuwNwzAyADP2\nhmEYGYAZe8NIEhUVFYwcOZqRI0dTUVGRanWMDMfi7A0jCVglJyOZ2KIqw0gTrJKTkUxsUZVhGIaR\nEDP2hpEEJk0aT17eNUSScjmVnManWi0jRaTD/I25cQwjSVRUVDBz5j2AY/yb468PUpbRsiRj/iYt\nffYisgLYBtQC1ap6bNQ2M/aGsR9ssrd1k4z5Gz/GviUKjitQpqqbWuBchtHmmDnzHtfQO8Zi926n\nz4y94YWWMPYAnp5AhmEYbYVJk8azZMlYdu92vjvzN3NaXI+WcON8BmzFcePcrar3Rm0zN45h7Adz\n47R+gp5zSVeffU9VXS0iXYFFwCWq+rK7Ta+//vq6fcvKyigrK0uqPobRGrEJ2symsrKSysrKuu83\n3HBD+hn7mJOJXA/sUNWZ7ncb2RtGE2jrxr6tX1/QpN2iKhHJF5EC93MHYCTwTjLPaRhtjYgbZ9Gi\nUSxaNIrTThvbrFjtdIj5bqhPkNdnNIKqJq0BA4C33PYuMLnBdjWMtsqCBQt0xIjTdcSI03XBggW+\n5YwYcbrCAwrqtgd0xIjTfeuUl9fdlfeA5uV1b5ZuQRDk9WUKru30ZI+TGo2jqsuBo5J5DsNIRxpO\nqi5ZMjYtJlUtjDNzaanQS8PIKII0qsOGHc2iRZdG9VzKsGFXN0O7d4DR7ucBzZATDOkSmtjWsdw4\nhpHmzJ37nPvpz26L7vPGsGFHA/cCo9x2r9uXOsrLy3nySWdV6YgR85r1BpRu8xHphI3sDSMJBDka\nX7lyDfAHopfbr1x5oy9Zixe/2UAWLF48jylTfIkLjPLy8ma7ktLVdZYu2MjeMJKAY1QvAOa57QK3\nzzv9+vVuUl9T2LBhY5P6WiOxrjPH6EfCOQ0z9oYRR3CugMHAXLcN9i1lxozJ5OZeRSRdcm7uVcyY\nMdmntBrgyjpZzuca37oZrQiv4TtBNiz00giIoMIcFyxYoLm5XetCE3Nzu/qSF3SIY7BhnJMUTnfb\npDYT5piOYaXJAh+hl2bsjVZPkP/JhwwZFhfzPWTIMN96BWGgg6StG8R0vOc6Z47W/aA2bgxEpB9j\nbxO0RqsnyDDHlSu/bFJfayUS+VKfmqBtTWAGMdEbCJ98AqecAh99VN+3aBEUF6dMJfPZG0YU/fr1\noKFP2+nzRjqnACgvL2fhwrksXDi3WYZx+vTplJQMoqRkENOnT2+WTm0iZLKqCq64AkTgwAMdQ3/1\n1VBd7Yzrhw9PrX5eXwWCbJgbxwiAIF0Tjs++s8JxCsdpbm5nX7LaegqAm266SaGw7p5Dod50002+\nZLV619Kzz2rUH1r1iCNUP/ssqafEfPZGphKkrzYIWUEb+3TzRRcXl8ZdX3FxqS9ZrfLBuHq16gkn\nxBr5xx5rsdObsTeMNCGoN4SIrHQb+aarsU/qQ7+2VvXmm2MN/Nixqjt2NOs8fjBjbxhpwoIFCzQU\nKqkz0KFQSdq8JQRBOrpxgnbnRWQdwy91K1Jv4Lt1U122zJfcoDBjbxhpQpAhnOlo7FUdg19cXKrF\nxaW+DX2EdHOd/eDb39fHOSZmFH/HwUeohsO+5AWNGXvDCIAgDE+Qbo6gFnpFy0sn/39QBGLsZ82K\nMfDPcpKWsD5tHrARzNgbRjMJyhUwZMjxCl2i3BxddMiQ433rFAp1UOit0FtDoQ5p4f9Pt5G972v7\n6CPV0tIYI//v3/wm7eZJojFjbxjNJChXQJATtEE+OIJKlxC0zz7IyewmPTT27FG9+OIYA6+TJ6tW\nV3uXlQLM2BtGM0nHyBDHJTRaodRto327hEpLB8c9OEpLB/vUKRg3VZAPs/0yf36sgR8yRHXFioS7\ntjVjb+kSDCOKdKya1K5dLbAIJw89wKW0a+dv2f26dVuAW4nOZ79u3XXN1LB5OPn6Y3Xym68/IatW\nwY9+BP/6V33f44/Dj3/c6CFtMje+16dDkA0b2RtpSEr9xwkoKOgbN4ouKOjrS1ZQI/Ig3ThBRi7V\nUVureuONsaP4885T3bmzSYenawRUBGxkbxjpQZDJ2XJycprU1xQmTjyXqVNjK2hNnOi9gtYUt7TV\nbbfd6Mq9uq7PKzNmTGbUqLOpqnK+O/n6H/Ili1dfhRNPpO7VrFcvWLAABvuvJ9Bm8Pp0CLJhI3sj\nzQhqRB7kyDDIUbSq6tixYzUU6qahUDcdO3asbzlB0qy3qS1bVE87LXYU/6c/NSsmPh1XLUeDTdAa\nRvMIMhonqBW0qsGFOaarEfNs7MNh1XvuiTXw3/teYPniVYMNLQ0aP8Y+6W4cEckGlgJfqur3k30+\nw2gOn332WZP69sfSpUupqdkN/BmAmprdLF261PcE39ChQ/n619+s++yXIN1LQeFpMvSDD+Dkk2HF\nCud7Vha8+CKccELgOk2f/sc6naZPv4ahQ4faBO2+GjAReASYl2Bbsh58huGLjh17xoUBduzY07Oc\ndF1Bm44Tj/vVac8e1Z//PHYUP3Wqak1N6nRKMfgY2Se1eImI9AZOBu4DJJnnMowgEMnBGfXOc9tY\nty91TJ48g6qqW1y9xlJVdQuTJ8/wJWvSpPHk5V1DpDiLE1o63pespBccefpppxBI+/Zw110wdCh8\n/rlje2+8EbKzgz9nW8br08FLA54AhgDDgPkJtifv0WcYPujZs2/cZGjPnt7DHIOcVC0o6JMg9LKP\nL1kR3Zrri05Whsle/E5fl5zYUfzcuZ5kBVV4Ph3nNiKQThO0wCnAHe7nssaM/fXXX1/XXnzxxaTd\nHMNoCkGuVg0q6iUo15JqekYbaU2NfjxmTKyBv+AC1V27PIkJ2kCn0wraF198McZWppuxvxn4AlgO\nrAZ2Ag822CeZ98cwPBPUAp8gDU9p6VHaMJ9NaelRvmQFZaQDkbNkiWpO1Ci+d2/Vd9/1rEugOrUS\n/Bj7pPnsVfUXqtpHVQcAZwAvqOo5yTqf0TpJt0LTM2ZMJivrQqAP0IesrAuZMWOyZzmxUS9OpMnM\nmff41KoKuBcY5bZ73b7U4dv3v3kz/OAHji/+W99yinHfdReEw/DFF3DYYclWPWNpyRW02oLnMloB\n6Zh/ZOnSpYTDIeAmAMLhS5sVMhkEmzfvBS7AmTAGuIDNm5/yJWvSpPEsXnwGVVVOSGhu7odMmvQX\nz3LKy8t58sk5dQ+wSZP28XdThT//GS68sL5v1Ch44AEoKvJ87sZIx7xG6URSo3EiqOpiVR3VEucy\nWg9Bjn6DekO47bbZOAnHxrrtD26fN4YNOxq4lMjIFy51+7xTVFTQpL6mEg5nAxOACe7nJPHee9Cn\njxMLf+GFkJMDL7/sGP+nnw7U0EP9A2jEiHmMGDEv5QOHtMOr38dxFzHCz3EJ5ATuyzJaD0GuVg0u\n6VgwkS9O2t58jRQcgXzfaXtLSw+Ni+wpLT3Ul6ykz0ns3u1MrkZPtk6bltSY+EyEFlxBOwvHqWkY\nvgnqtTvIVaHdunVm+/Yro3qupFu3np7lfPzxZ0A+EXcQXOn2eWfduu3Uv21E+vylJV658ssm9e0P\n556PIeJaOmn3Nyg/6aT6Hb7xDZg7Fw44wJeeRvA0auxFZP4+jitJgi5GhlFeXs6UKZdEZU68JOWv\n3QMHHsinnx4ERPKpD2PgQO/TTc5CrJ9S72cfi8gTPrVKtB7R3xrFoqJ8Nm2KfZgVFXl/mH322ccU\nUskUvsHVPFe/4amnnAlYj1RUVET5/8en/HfQJmlsyA9sxomVL4tqw9x/13l9hWjkHMl91zHSmqDc\nL0HXVQ1iMVRQFaFUg63k5LiE8jVSAhDyvbmEamtVX3hBHwvl62by9DnK9U4maDvu8b3QK8iyhJkC\nQcbZAwuA7zSy7WWvJ2pETtJuhpH+pGMJwKB82kEW5AjSGDqLxmJj9pu0aGzFCsf33r+/6hFH6LX5\nJVrCH2Our3llCQujHkCFyStL2EbwY+wbdeOo6kn72PY/zXufMIxgKS8vD+TVPyifdpcu8Z7ORH1N\noby8nHnz/hLl5pjm+1r79evNpk3xfQnZvRuefBJmzYK33oIzznD88EOG0PHmm9k4dQoQiQryVwQF\nIvMbuTgRQtCc+Q2jcfbls78TeFRVl7SgPkYG4cR8x1YomjTJZ4WigGjXrhonZDLCpbRr19mznHSN\n+R49egTLlv2W6Hq2o0dHGWlVeOMNmD3bqdN67LFwwQWOH759+7rdgqxU5cxvzCB6Alrkl75kGfug\nsSE/cDnwKrAS+C0wxOtrw/4a5sbJaNLRV5uX1yvOzZGX18uXrKBy47RIiuM1a1RvvVX10ENVS0tV\nb7pJ9fPPfevshaTUoG3jkIzcOEB/4FpgGfARcD1wkNcTNSI7mffDSHPS0WcfCnWLM/ahUDfPcpyJ\n3tiJ0HQoyB19z0NU6Sgu1Ve69lTt1El13DjVxYubVc7PD0E+zNIpeVk0QeuVFGOvscZ5CPAWUOv1\nRI3Ia/ZFG62XIBdVBWUsiou7xkXjFBd39SwnL69rXARNXp53OY5OwRZCObpdid7CSbqaQn0lK0ff\nueIK1W3bfMkLiiCMYbqmJU6GXska2Ydwsi89CqwF/gL8wOuJGpHdrAs2WjdB/SdwRr6xo3G/I9/c\n3O5xhjU3t7tnOVASJwdKfOkUSOjl5s2qd92leuyxuikvT38t+fq17OK0KTgeBOma9TIZevkx9vua\noB2Jk63ye8C/gceA8aq6o/kzBYbhMZnWPvj44/8C7wG3uj1X8vHH/qpLVVXVAO8Ao92eAW6fN/Ly\ncuomZ6P7/DBjxnWMGhWdvKyGGTOasII2HHbqs86aBf/4B4wYwV8OPpgx//6AWv4ItfDBnEs58MAD\nfU+uGq2Ixp4CwAs4qfaKvT5Bmtqwkb0RAHl5PeJGTnl5PXzJys5uH+drz85u71nO2LFj49xBzZ2k\nbbKbY/ly1euvV+3XT/XII1V//3vV9etVtRlx9kkmCDdOkNXBgiTt3TjAiVGfBzTYdrrXEzVyjmZd\nsGGoqhYU9I0z9gUF3ksJqqrm5hZrwwU+ubnFnuU4r+7HK3Rz2/HJdSns3Kn60EOq3/mOakmJ6iWX\nqL75ZtxuQc4lBEWw1bNiH2Tp4MZRDaYUZDRBG/tliT4n+u63mbE3gqBnz/5xo/GePfv7kpWV1Umh\nOMoYFmtWVifPcoJMl6DayMg3HFZ99VXV8eNVi4pUv/td1b/+VXXPnkblBPkWFFSESVpVz0oC6TKy\nb8niJYaRFDZu3Aq0p34F5kS3zzuhUDuqqn5L9AKfUMj7ytB167bgzCGMjerzl6myoqKCU045i5qa\nmQC8/8KZ/HPc6Rz8r385lZ7OOw/eeadJGSbz8jokmEvo4EundCs8k64L2YLMytoczNgbrZ6qqixg\nJtGGtapqki9Z2dnxxTwS9bUkF110LdT8hh/QifOYxf/U7uX5vz7Jwc8+Dccf75T4ayITJ57L1Kmx\nK4T9pDkI0oAFZaSDmvBvszQ25Ae24uRnnQ9scf+NtC1eXyEaOUezXmWM1k8Qvszc3G4JwiW9L4Ry\nZBXHuV/8+Ox79hyo0DHKtdRRe/Yc6F2hd97R30merqFQF/M/Oo5Z2oE/a3a2fz97EPc8aP94ui6G\nCoJ0cePsyxCX7aMN83qiRs7RrAs2WjfBpRMOrpITFMUZMSjyLKd9++I4ndq3b+JDY9Mm1TvvVB06\nVPWAA3Q67XQQRTEPID/zCEGSrpEv6Upar6AF+nkV5vnkZuwzmqBWhgY5QQvt44wYeA+9zMqKX1SV\nlbWPRVW1taoLF6qeeaaTuuDHP1Z97jnVmhotLj5AG5Y4LC4+wNf1qQZjeNJ1MjRT8GPs9+Wzfwon\nPQIiMldVR+9jX8NIGWvXbge+i5PJA+C7rF1b6VNaLjCC+kpVI4BFnqWIZDWpj88+gwcecFqXLs5k\n6x//CCX16ZB37IisY4ykIt4U1eeNdJxYNVqGpk7QDkyqFkZG8v3vf4s5c2InC7///dM8ywmHd+EY\n5Pq0veFwtU+t1JV1qPt9kdvnjf79u/Lpp7HX1r+/a6x37nTyws+eDe++Cz/9KcybB0cdlVBWVVUY\np55tfb73qqq9nnWC4CZWhw07mkWLYq9v2DB/+eyNliHBUMMwWoa33/4EqAGmuq3G7fNKDvUFuce6\nn/2lJoBwE/v2zR133AbsAia6bScPX/S/Tm74Pn2cXPEXXwxffgm//32jht4hh/owzrHuZ7/XFwyL\nF7+Js8B+ntsucPtSS0VFBSNHjmbkyNFUVFSkjax0YF8j+yNEZLv7OS/qMzj+osL9CReR9sBioJ17\nrr+p6jS/yhpti5Ur1wA/B5a7PQNYufIpH5IS/Yz9RhULDasmwR7PUm699VYgnx78krP5F+fxNzpM\nnQrXXeeM5nv18iCttol9+yfYEflg6vMRzaH+75gagnRRtUl3l1cnv9cG5Lv/hoDXgG9EbUvO7IWR\nVIKKLHDCE2NTE/gJTwxqUtWR1Tlu4hE6exOyd6+eRgedz5G6ic56L+frN5niXU6dTh214ape6OhL\nljOxOlqh1G2jU55WOiiCnDRO9wlo0nEFrarucj/m4rx7en8nNtKGiooKRo06m6qqWwBYvPhs5s17\nyNeIZ/36jU3q2z+51LsUcD/f60NOM3n7bccP/8gjXMYeZnEMP+EVdtEBZ+Trj6ysdoTD46i/vnPJ\nynrAl6wNG9bi1CCqzxC6YcPBPjWrBv4c9dk/FRUVUYuhxrfuEXS64vXp4LXhzAu8BWwHZjTYlrQn\nn5Ecgqya5IxYYxcw+RmxBjIar5PVwY21j+hUpNCh8QM2blT9059Ujz5atXdv1alTVT/5xM2xE3tt\nfmPjhw8fHvfmMnz4cF+ygvr7BV1lLIhFR0EuXkrXQigRSNORfRg4SkQ6AU+KyGGq+l5k+7Rp0+r2\nLSsro6ysLNkqGc3gk08+a1Jf08jBmXCMjFjHArM8S8nNDVNVFeuHzs31qRJ7cKJvIiPWvW6LorYW\nnn/eGcUvWAAnnQQzZsCJJ4KbWqFduzx27z6L6Gtr1+4RXxqJFOL4xyO+9cFuX9sgqAihINMllJeX\nM2XKJVEF1S9J6dtGZWUllZWVzRPi9enQnAZcB0yK+p6cx56RNDp27Bk3Yu3YsacvWUGN7J0Mk7GL\njvxmmHT84Q1X0LorXz/+WHXKFGcE//Wvq95xh7PaNQF5eV3iri0vr4svnYIcRQdS9UqDLRafjgXH\nbWTvERHpAtSo6hYRycNZofLrZJ7TSC4HHngQy5YNJXrEeuCBS33JKigoZPv2m4lOYFZQ8AvPcnbt\n2kvDRVW7dr3jSydnFD+HiE+7A5P4ITtg2DD44AM46yyn6tMRR+xTSnU1NHxrqa7257cPNptjKE4v\n8Pf3c97MIlFLV/mUAU747ZVR368E/M4jBIPztjGGyH3avXtMSjJVBkmy3Tg9gTkiko3ju39cVZ9N\n8jmNJDJjxuSYCdrc3KuYMeMhX7J27NjVpL79sWbNF8AaohdVrVnjd8IwBziBb/ILzmM7p7OHJShc\ndhmccgpN9Q/V1OwBZgO3uT0Tqamp8qVRsC6FGqIfZn4N68yZ97i/AedBXVXlP+tlly7dgeOIfgB1\n6bLcs5wgcSayXyKYiez0IKnGXlXfAY5O5jmMlqW8vJx58x6K8ov6i8QBUN0DXEi9f/xtnCker3La\nA+cRHY21bbx9AAAgAElEQVSj6t33z6pVXMMOzuVJwrRjNt04lPWsQdHTT/cujxrqr817HdsIFRUV\nTJ/+x7qY7+nTr2Ho0KFtxrBOmjSexYvPoKrqEAByc19g0qS/pFQnxzTG1iNwHt6tF8tnb3imvLw8\noNfZELELmCYCfka/1cSPVps4st+7F+bPdyZb//UvBgLjKOQ1bsdZYOVvUVVubl7MyBfmkJvrz9WR\nnNzxzoPDr0so+EIhQbmEgqFLl5Im9bUqvDr5g2zYBG2rJKh6miLFcRNzIt5zx4dC8RkmQ6F9ZJhU\nVV22TPXSS1W7dFEtK1N98EHVHTu0fvFSdBind50KCnrFySko6OVZjmrwC3yCWhSXbmUJg8QmaI2M\nZ/r06UydejPgTFA6n2HKlCmeZWVnCzU17wCRhKoDyM5uetWlCDk57aipie+LY+NGePRRZxS/YQOM\nGwevvw4Do/P8BZOaYPv2zUBsOOj27f7mESZNGs+LL46mpmYqAKHQZiZNmutLFgT5ZtZ2aZNVr7w+\nHYJs2Mi+1RFk6KVIftxiIZF8z3JCoY5xckIhN4SzpsbJC/+jHzl54s8808kbX1OTVJ32GcLpkXQs\nFJJJC5jSEYIsXtISzYx968MpyhFrxPZZlGMfBOUycVbQxuo0iALVyZNVDzhA9ZhjnMpPjcTERxMK\nddaG+XpCIe+rcYuL+8VdW3FxP89yVFULCvomcAn19SUrKNLVtZQp+DH25sYxPBEO76HhZKjT54eg\nsjlmAYPpwDR+xBOcx685iB3O5OuCBXD44U2WFA5nA7cTPbEaDnsvXv7oo3fz3e/+BGdMAyKX8eij\nj3uWA7B7d/z9TdTXkmzYEJ/DKFGfkT6Ysc8Qxo0bxyOPPAfAWWd9lwceeMCnpOjc6hEu9ykr0WIa\njyGKqnyLrZzL+ZzG+bxMITPZybNkUTVzpmeNnEFT7DyCRiy2Z/ZQf33+jXPXru1ZvTrW/9+1a2ff\n8oIhuIVQbTKdcBpixj4DGDduHHPmPElk0VGkOpQfg5+VlU04HN/nj0S5ce5v2qFffgkPPgizZ3M3\nYWaTyy/4DWvpjDMx6m8Bk8hOVO8leoGWiPeqUOeeOwHVdkTegFQv5dxzJ7BqlfeY9h49BrB6dQ3R\nuXF69PD/XzeIDJNBxusHGVpq7AOvfp8gG+azbxGys+NDE7Oz/fnZnQyMscW9/WZg9JyHfs8e1ccf\nVz3pJNWiItXx41Vfe00TZ70s8qlTUPMI8fcc/N3zoPLZqKZnhsl0DL1MdzCfvZGI2gRu8ER9TWH5\n8lU4L4SRBTCXun1+yMPJqBHxifcGVsfvtmwZzJoFjz0GRx4J557r1HDNz3d38B6u2ThhGrpx/JRg\nyMrSBG9Aft1Bwa3mTMcMk8Ev0DISYcY+A+jZsxOrV8f6V3v27ORL1mefRXLQjI3qm+hLluMy+ZJY\nl4nr296wwYmJnzULtmxxYuLfeAMGDEggaS8NY9r9pyfYjVP4pF6nuBTHTaB7985x97x7d39+9nRd\nzRlUvH6bjGlPQ8zYZwCzZ9/JySf/kHDYydOSlbWH2bMf9iVLE0xWJuprmqwOwO+IPDiyqWWkXgY/\n/KGTL/6UU2DmTPj2tyErq1E5ubkhqqpi89Dk5vr9aXfASV4WPYr2/jA7/PCjWL16ANE+7cMP9+fT\n7tWrgIYPs169TvMlK11H0bbQK/mYsc8YqoFPoj43R0608ZvYDHmO+2U5/fk7p/MTZvMlu2HECLjv\nPujctJFwu3adqKr6MfUFr4+nXbu/+tQp0YPL+8PMKez9W6LfEPwW9p4/fwkNyy7On++nMLuNojMZ\n8TsqC+TkIprK82cKgwYdxqefxrpLSkt788kn7+3rsIRkZ3cgHK4GIm6grWRl5VBbu9OboIsvhjvu\niOk6jA68TxWq3iJpCgq6s2NHmOjY/44ds9i+fa03nYC8vAL27Mki+l61bx9m9+7tnuQcfXQZy5Z1\noT7H/lEMGbKBN9+s9KxTSckgNm26jujY/+LiG9m48ZN9HWa0YUQEVfU0WWUj+wxg5coNNPSzr1zp\nb5QZDtfihEwOcnvedvuawM6d0LFjXPdxdOJ1coCtFBd790Xv2LEX+D3R17djx2We5QDs2ZMLnEv0\nKHrPHu+ToR9+GHmQ1j+APvzQl0pMnHguU6fGunEmTvT39zMyFzP2GUBeXnu2b4/v80cO0J76aJwm\npACWxAOQUHZ7amsVKHB7trN16xYfOiWK8/cb+w9OvdeIkfbnz96zJ0xD3/+ePf4msiNJ5uqLl1zt\nK/FchCDi7IOUY7QQXmM1g2xYnP1+CSJnSJCJtKCoaTHtjz2mUTvVt9raKFnBJAuDDnFx6NDB1/UF\nlQgtO7trgrUNXX3pFCRBxtnn5natk5Ob29Vy2rQgWCK0tkWQC1ecxVAlCiW+F0GpapQhjDb2hc7G\ncDixgb/zzkZkFUQtYnrA/VzgQ6fsuIVekO3r+oIqXt6z58C4B1DPngN96aSafrnj07FIeCZhxr6N\nEdR/zGBH9qE4WVsTGfgm/G2doiOxI/v9Fh1JqFNHt0WMvfPdDwsWLNBQqFOdrFCoky/j6qx6jdUp\n1ateHb2CMdLFxaVxcoqLS33pZHjHj7FvPHjZaDPcdtts6kP35gEXuH1+yAcu4Gs8jDIOZRuF0Zs3\nbar//78fwuFI8etRbpvj9nklB/gT8Krb/uT2eae8vJxp066iuHg9xcXrmTbtqmbkjvkZ0MttP3P7\nvBO76tVJGBbxlXsnksBsjtt8JJ8Diory4+Q4fUa6YhO0SSCoiaugFsBUVe3CWRV6qNvzPFVVHXzp\npGwDYrNJziWH0R7DJQE3nUBsGoBw2HsUjROGFpviQBqZFN4fFRUVTJs2g5qaIgCmTZvhq7h3ULVe\ngyaoBGaFhYXASuoXslW5fUba4vVVIMhGG3TjBF11Z+zYsRoKddNQqJuOHTvWl4zi4gPifOPFxQd4\nUSKhm6beP76P5GX7IHECM++FQtq3L47z2bdv768qVGnpoXFuqtLSQ33JCrLWa7pVhXJcjLEuOEte\n1nJgPvvUE2QGv6B87b4Ke2/dmtDAH0VeYP5x59iiqOsr8iXLeZjFToZ6ephFkc5RNEFVcgpClpUS\nTC1pZ+yBPsCLwHvAu8ClDbYn8XakhiCNfVCTYJ5G0PuZbHUMdMMwR7/Gvl3ciBzaeZYTCnWLu75Q\nqJsvndLV2KcjVkowdfgx9sn22VcDV6jqWyLSEfg/EVmkqh8k+bwpw8mJErva0W9OlOCowUmkFfGv\nvk9M2t7Zs+G88+IPC4cTLIjKAYYBN7rfhwEv+NSrPbErX+cA3n32eXntEiwaa+dLo/79u/Lpp7F/\nv/79e/uS1dax5GWti6RG46jqGlV9y/28A/gAJzShzbJ48ZvACBxjeCMwwu3zzsSJ5+IY6UjEw6Vu\nnzdCoWzqc9BPAEKEsrMcQy4Sa+hnzaof1Cac5NwFLAKuc9sit88PiX5+3n+S11zzMxreJ6fPO3fc\ncRuhUC0wFZhKKFTLHXfc5kuWYaQTLRaNIyL9gSHA6y11zlSwYcNanOIXkciXRWzYMNiXrKFDhxIK\n1VJTMxWAUKiWoUOHepaTl1fI9u03AmPRSKGPhulsHLdaE8gnOi2xwxWedQLIyqomHI7N+Z6V5T2D\n5pQpU/j444955BHnDeqss07znU6gvLycZ56Za2kAjDZHixh714XzN+Ayd4Rfx7Rp0+o+l5WVUVZW\n1hIqJY1t27YBuUTnjnH6vDNz5j3U1NxBxLDW1MzxVVXoqPbZvLR9HDAudsPWrZDCcLl27XLZvXsX\nzigaYBft2nnP2VNRUcFf/7qAmprfAvDXv17DmWdW+DbSQbknLHeMERSVlZVUVlY2T4hXJ7/XhuPk\nrQAuT7AtCVMXqSXIlYXNXu2YYKL1Zr6n0EVDoU6+dAKJixByMmX7kRXMytd0DAO0aBUjmZBuE7Ti\nrGy5H3hfVW9P5rnShX79erNpU3yfH7ZtW0/DCkXbtu1H1hVXwO3xt1qYRH1xj7HU1NzvSycoAjpS\nX8CkM/5fEMPEZtCcyH4zaCbAcZ0tBA53exayYcORPnUKhqBqvRpGUCTbjXM8MAZ4W0SWuX2TVXVB\nks+bMkaPHsGyZbEGevRof9E4mzfvpX6yF2AEmze/Fb/jtm3QKUFN2Y8/hkGDECmg4QpaP0W0AXcO\nYRdO+l6AK90JTT/kEl8C8HLPUtasWU3DtMtOn2EYdXh9FQiykWZunCDihp0VmLGx435XYDoZGAuj\nZBXGZmBMFA8/dGicnKys+OySWVnes0uqqhYU9IlzLRUU9PElKyurJE5WVpb3RGhZWV0SyOniS6eg\nMDeOkUxINzdOa6KiooLTTqvPZbJkyViefNJ7fc4VK9YDdxIdO75ixVXN0Cx2svfkHVsSh0QmjImP\nbMqi4QjaTw4agI4d89i+PTYPTceOeb5kde/eidWrY6NxundP8IayHxJVympy9awkYbVejXTDjL1L\nUD5W1Xj3SKK+prBu3VZgLMLThHnS6Ywuq/qPf8DJJzdBUjDx7ABr136O4xKqr9G6dq33JGgOYZwY\n/chir1048wHeaN9e2LMn1nXWvn3qf9q26MhIJ1L/P6KNkZ29l3A41vBkZ/sz9o9t/4rvNcgwCXiI\niY+wg4YTvbDbl07hcD7xbwn+yu2tXbsN+Dn1E8fHs3btbM9ypk6dyNSpN1IfwrmXqVP9r1q2kEmj\nTeLV7xNkI4189kH5WB2fdmwYoCef9n//m9AX3457XP99jmednNw4Dcv/ec8u6cgqSiArQVnCJsmK\nT9DmpyyhajDZQVXN1260Dki3RGj7PXkaGXvVYCZofcfGJzDw/0t+AmPo3bAGa6CzE8TZ+ysBmJfX\nRRsmVcvL8z6xGqSBDjKRnWEkCz/G3tw4UQThY3VCLy+k3g/9NqNH/yLxztOnw9Sp8f2um+Y+KW6W\nLvVso6GfHXb6lFUA3E5zwyUBpky5vIH7ZTtTplznWY7FtBvG/rGyhFFMnz6dkpJBlJQMYvr06b5k\nzJ79KA2Tjjl9Ltu21Scgizb0X32VoJzfNhom+HL6vFKIY+jHuu0Pbp93cnPjs0km6ms67YCb3NYc\nOcEwadJ48vKuIXLPnQpT41OtlmE0H6+vAkE20siNE1ShkOzs+CLa2dklCd00esEF+5Ql0lEhy/Vt\nFytkqYj3dAL1BUKa5w5SDbZQSEFB3zi9Cgr6epYTtJ/d8rQb6Q4+3DiiMSPJlkVENJXnj6akZBCb\nNl1HdHx8cfGNbNz4iSc5WVmd3MH5oZzCZubzUfxOTbzmrKxcVPOIdr+I7CYc9hbqmJdXyJ49tcAR\nbs/btG+fze7d3t8SsrM7uzHskdQE75KVlU1t7RbPsnJyurvJy+rveSh0NdXVa/d1WEIsgsbIJJy6\ny+qp0LK5cQJGtBZlG8prsYb+tdcSuGn2jWoBDd0vTp83pk69hoauJafPO+FwmNifTZbb551+/brQ\n0E3l9BmGETheXwWCbLQlN84VV8S5aebzPfWbAkBVXddNwyga76GJpaVHxblLSkuP8qVTbm582b7c\nXH9l+xYsWKChUAeF3gq9NRTq4MttYuGSRqZBJoZeBulf9RyrvWJFQl98NvcHYgxzctrFPYByclJb\nozXIB4dqMH+/thAuCVizlrA19ntRj/a2VYdeBpXPJiLrsceepabmFgAee+yqxgtgJMpBs2ABlJeT\nn9+V2t1XAtnuhivIzs6O378JVFd3oOFq1epq76tVQ6EaampiV9CGQv7+9IWFHYDYfDaFhQf7kgWW\nUiAaTZP5KyN9kEbyXfmhVRv7IOOrJ0+eQVXVLXWyqqqcvjpZs2bB+efHHjRokJNGOAbBqfkXibOv\nJdW3uaioC6tXfx+Y5/ZcQFHRfF+yunTpDvQguuB4ly6pNVKTJo1nyZKx7HYzQDjhknNSqpNhpBut\n2tgHycqVX8b1bVzxeeJR/KZNUFTUiKRs6idVwZl4vNaXTjk5O6mujh2R5+Ts9Sxn795aYDBwa51O\ne/c+5UunXr0KgCeJjhDq1es0X7KCwjJMGsb+adXGPsgRXb9+Pdi0yXFPvME0hrICNkft8Ic/wCWX\n7FdOKJTbpL6m4LhxziV6RF5dPduznOhrc7iSfv38uV7mz19C7MMM5s+/sdH9WwpzBxnGvmnVxj7I\nEd0fJpzDt372M+KKcnv0o3br1ont22MNa7duPX3p5LiEYkfkTp83Zsy4jlNO+Qk1NY5rKRSqZsYM\n72kJAKqr42P8E/UZhpFetPo4+/LychYunMvChXO9G/pwGHr1AhHX0DuM++ZIKhYs8Gzo66nC8dn/\n2f3sj+HDh9AwDt3p805WliT87JVu3TrjTNBGdLrS7TOM9OTll1/mkEMOCVzuihUryMqqX2dy8skn\n89BDDwV+nsDwGr4TZKORsKKkc++98SGTd90ViOji4lKF0QqlbhutxcWlvuUNHz5coUShRIcPH+5L\nRpChiY6s2Nj/1hbmmI6k7P9CE+nXr5/m5eVpx44dtUePHjpu3DjdsWNHqtVKKcuXL1cR0dra2qSd\no7HfBT5CL1v9yL7JrFlTn4Dsggucvq5doabGsX8TJuz7+CZSVNQOeA7o6rbn3D5/LFq0CNUNqG5g\n0aJFzdAsUkpwtPvZH06isIeBUcAo8vIetkRhGYCI8Mwzz7B9+3beeustli1bxowZMwI/T01NTeAy\nDYe2b+zLyx0D3zPKb/7OO46BX7cOfMbAN042kE99aoJ86mPuU8OwYUfjpDge5bZ73T7vROZJRoyY\nx4gR83yvazBaL927d2fkyJG89dZbdX2vvfYa3/zmNykqKuKoo45i8eLFdduWL1/OCSecQGFhISNG\njOCiiy7i7LPPBupdIbNmzaJfv34MHz4cgFmzZnHooYdSXFzMSSedxOeff14n74orrqB79+506tSJ\nI444gvfeew+AZ599lsMOO4zCwkJ69+7NzJlOlbfKykr69OlTd/wHH3xAWVkZRUVFHH744cyfXx+G\nPG7cOC666CJOOeUUCgsLOe644/jss8+adF/Kysq4//77AXjggQf41re+xVVXXUVxcTEDBw5kwYIF\ndftu3bqV888/n169etG7d2+uu+4632lHmozXV4EgG8l6dV24MN5NM2nSfg8LYjWn48aJdXM0x40T\nBG1hhWlbJ2n/FwKif//++vzzz6uq6hdffKGDBw/Wyy+/XFVVv/zySy0pKdHnnntOVVUXLVqkJSUl\numHDBlVVPe644/Sqq67S6upqXbJkiRYWFurZZ5+tqvWukLFjx+quXbt09+7d+tRTT+mgQYP0ww8/\n1NraWr3pppv0m9/8pqo6/0e//vWv69atW1VV9cMPP9TVq1erqmqPHj10yZIlqqq6ZcsWffPNN1VV\n9cUXX9TevXurqmpVVZWWlpbqjBkztLq6Wl944QUtKCjQjz76SFWdVfQlJSX6xhtvaE1NjZ511ll6\nxhlnJLwnDd04ZWVlev/996uq6uzZszUnJ0fvu+8+DYfDetddd2mvXr3qjj311FN1woQJumvXLl23\nbp0ee+yxevfdd8edo7HfBT7cOK3e2EcM9CnfGRVv4EF1584mywkiv0pp6WBtmAK4tHSwZzlBYsY+\n/WnS/4VEv2+vzSf9+vXTjh07akFBgYqIDh8+vM7g/vrXv64z3hHKy8t1zpw5unLlSg2FQrp79+66\nbWPGjNExY8aoar3BXL58ed32k046qc5oqqrW1tZqfn6+rly5Ul944QU96KCD9LXXXovzlfft21fv\nvvvuOr0iRBv7l156SXv06BGz/cwzz9Rp06apqmPsL4hKPf7ss8/qIYcckvCe7M/YDxo0qG7fnTt3\nqojo2rVrdc2aNdquXbuYe/Loo4/qt7/97bhzBGnsk+rGEZFZIrJWRPw7ifdBJF3CaYs2M/+FedEb\n6n/e+flNkhW7GtdJwRAJ6fRCYWExTqjkWLfd6valDivI0UYIwtz7RER4+umn2bZtG5WVlXzwwQes\nX78egJUrV/LEE09QVFRU11555RXWrFnDqlWrKC4upn379nWyol0qifpWrlzJZZddVierpKQEgFWr\nVvHtb3+biy++mIsuuoju3bvzs5/9jO3btwMwd+5cnn32Wfr3709ZWRmvvfZa3HlWrVoVd/5+/fqx\natWquuvs3r173ba8vDx27Njh65716NGj7nO+a4d27NjBypUrqa6upmfPnnXXOGHChLr7mSyS7bOf\nDZyULOERA/177uQOLkR4gJEjToeRI31KbP4kZpcuJU3qa0nMz24EyQknnMC4ceO48kpnPUnfvn05\n++yz2bx5c13bvn07V199NT179mTTpk3sjqx8hBj/e4ToHDB9+/blnnvuiZG3c+dOjjvuOAAuueQS\nli5dyvvvv89///tfbrnFyWc1dOhQnnrqKdavX8+pp57Kj3/847jz9OrViy+++CLiWQCch8sBBxwQ\nzM1pAn369KFdu3Zs3Lix7vq2bt3KO+8kZUxcR1KNvaq+TOw61KTwEYdwMXc0S0ZQk5jpOopu1noE\nw2jA5ZdfzqJFi3j77bcZM2YM8+fPZ+HChdTW1rJnzx4qKyv56quv6NevH0OHDmXatGlUV1fz6quv\n8swzz+wzwdeECRO4+eabef/99wFnMvOJJ54AYOnSpbz++utUV1eTn59P+/btyc7Oprq6mkceeYSt\nW7eSnZ1NQUFBwgSE3/jGN8jPz+e3v/0t1dXVVFZW8swzz3DGGWcALZOMrmfPnowcOZKJEyeyfft2\nwuEwn376KS+99FJSz9uqo3GCNKyLF79Jw0IhTp83bBRtZAJdunThnHPO4cYbb6R37948/fTT3Hzz\nzXTr1o2+ffsyc+bMuuiSRx55hFdffZWSkhKuu+46fvKTn5CbW59CpKHhP/XUU7nmmms444wz6NSp\nE4MHD6aiogKAbdu2MX78eIqLi+nfvz9dunThqquuAuDhhx9mwIABdOrUiXvuuYdHHnkk7hy5ubnM\nnz+f5557jq5du3LxxRfz0EMPcdBBB9Xt11CffT2YGtu2PzkPPvggVVVVdRFHP/rRj1izZk2j5wmC\npJclFJH+wHxVHZxgm15//fV138vKyigrK/MkP6hydCNHjmbRolFEJzAbMWIeCxfO9SXPMLzglplL\ntRotwk9+8hMOPfRQov/vG4mJ/C4qKyuprKys67/hhhtQj2UJU27s0+UH3jA3fl7eNTYqN1qMtmzs\nly5dSlFREQMGDKCiooLTTz+d1157jSOPPDLVqqU9jf0u/NSgbdWJ0IKkvLycKVMu4bbbnAyOEyde\nYobeMAJgzZo1nH766WzcuJE+ffrw5z//2Qx9CkjqyF5EHgOGASXAOuCXqjo7antajexHjTqDqion\nYVJu7ofMm/cXM/hGi9CWR/aGf4Ic2SfdjbPPk6eRsT/66G+xbNlH1KcTvpIhQw7mzTeXeJYV1DyC\nkTmYsTcSYW6cJLBy5RrqF0NF+rwX5QiyLq5hGEZQtOrQyyDp1693k/r2R1ArcQ3DMILERvYuM2ZM\nZtSos6lya43k5l7FjBlpXIjAMAzDA+azjyIIX7uFcBp+MJ+9kQiboE1zbILW8IoZeyMRZuwNo42R\nzsa+f//+rFu3jlAoRHZ2NoceeijnnHMO48eP32cqAXCKkwwcOJCamhqysmyK0CtBGnu7+4bRilFV\nfv3rW+nadQBdu/bn+utvCvyhESlJuG3bNj7//HOuvfZafvOb33D++ed70tNILWbsDSON2bRpE6ee\nehY9egzimGO+w7vvvhuzffbsOdx442w2bJjHhg3Pcuutf+cPf7gzTs7SpUt54okn+PDDD5ulT0FB\nAd///vd5/PHHmTNnDu+99x7/+Mc/GDJkCJ06daJv377ccMMNdfufcMIJAHTu3JmCggJef/11Pv30\nU77zne/QpUsXunbtypgxY9i6dWuz9DKagNdqJ0E20rwUm2G0FIn+L4TDYT3mmDLNzf25wocqcrd2\n7txT161bV7fP8OGnKzwWVZ1kvh53XHmMnCuvnKL5+X20sPA0zcvrpvfdN9uTbv3799d//vOfcf19\n+/bVu+66SysrK/Xdd99VVdW3335bu3fvrk899ZSqqq5YsSKmmpOq6ieffKLPP/+8VlVV6fr16/WE\nE06oK3FoxNKYjSTdKlUZhuGfzZs385//vElV1R+Bg1EdTzh8FK+88krdPsXFhYgsr/suspzi4k51\n3999913uvHM2u3YtY9u2v7N790tcdNFl7Ny5s9n69erVi82bNzNs2DAOO+wwAAYPHswZZ5xRV3Bc\nE7hvSktLOfHEE8nJyaFLly5cccUVMQXKjeRgcfaGkaa0b9+ecLgap/5PFyBMOLyWDh061O3zq19d\ny3PPncDu3V+hGiIv71FmzHi+bvsXX3xBTs5hOOmpAA4mO7uQ9evXx8jxw1dffUVxcTGvv/461157\nLe+99x5VVVXs3bs3YZWoCGvXruWyyy5jyZIldcU7iotTW7ozE7CRvWGkKfn5+Vx22eV06HAicAvt\n25/GIYcUxNR8OPjgg3n77df51a8O4IYburFs2b844ogj6rYPHjyY6uplwL/dnifIy6PZZfjeeOMN\nvvrqK44//nh++tOfcuqpp/Lll1+yZcsWJkyYUFe4JFG0zi9+8Quys7N599132bp1Kw899FDd/kby\nsJG9YaQxt9wynaFDj2DJkn8zcGAZF174c3JycmL26d+/P5MnT054fO/evXnssVmceeZJ1NZCYWEB\nzz33VJyM/RFxx2zbto2XXnqJyy+/nLPPPpvDDz+cHTt2UFRURG5uLv/+97959NFH69aWdO3alays\nLD799FMOPPBAwCm63alTJwoLC/nqq6/qasgaycXi7A0jDUh2nH1NTQ2bN2+mpKTEc7z7gAEDWLt2\nLaFQiKysLA477DDGjBnDhAkTEBHmzp3LpEmT2LRpE8OGDWPAgAFs2bKFBx98EIDrr7+eu+66i5qa\nGvIPDPMAAAbqSURBVBYsWEDHjh0555xz+OijjzjwwAMZM2YMt99+e8JC5JmOLaoyjDZGOi+qMlKH\nLaoyDMMwPGHG3jAMIwMwY28YhpEBmLE3DMPIAMzYG4ZhZABm7A3DMDIAW1RlGGnC/nLDG0ZzSKqx\nF5GTgNuBbOA+Vf1NMs9nGK0Vi7E3kk3S3Dgikg38CTgJOBQ4U0S+lqzzBUllZWWqVYjDdGoaplPT\nSUe9TKfkkUyf/bHAJ6q6QlWrgb8AP0ji+QIjHf+4plPTMJ2aTjrqZTolj2Qa+wOAL6K+f+n2GYZh\nGC1MMo29OSENwzDShKQlQhOR44BpqnqS+30yEI6epBUReyAYhmH4IG2yXopICPgIOBFYhVM94UxV\n/SApJzQMwzAaJWmhl6paIyIXAxU4oZf3m6E3DMNIDSnNZ28YhmG0DClJlyAifUTkRRF5T0TeFZFL\nU6FHIkQkW0SWicj8VOsCICKdReRvIvKBiLzvzoWkHBG5wv3bvSMij4pIuxToMEtE1orIO1F9xSKy\nSET+KyILRaRzGuh0i/v3+4+I/F1EOqVap6htk0QkLCItWvG7MZ1E5BL3Xr0rIi2+CLORv99RIvKa\naxfeEJFjWlinhPbS6289VblxqoErVPUw4DjgojRacHUZ8D7pE030e+BZVf0acASQcleYiBwAXAJ8\nXVUH47jpzkiBKrNxFu1Fcy2wSFUPAv7pfk+1TguBw1T1SOC/QOKCsS2rEyLSBxgBrGxhfSCBTiLy\nbWAUcISqHg7cmg56Ab8FrlfVIcAv3e8tSWP20tNvPSXGXlXXqOpb7ucdOAasVyp0iUZEegMnA/cB\nKU9U4o4A/0dVZ4EzD6KqW1OsVoQQkO9OxOcDX7W0Aqr6MrC5QfcoYI77eQ5waqp1UtVFqhp2v74O\n9E61Ti63AVe3pC4RGtHp58AMdxEmqro+TfQKA5G3sc608G+9EXt5AB5/6ynPeiki/YEhOP8JUs3v\ngKtw/rjpwABgvYjMFpE3ReReEclPtVKq+hUwE/gcJ9Jqi6o+n1qt6uiuqmvdz2uB7qlUJgHnAc+m\nWgkR+QHwpaq+nWpdojgQOMF1mVSKyNBUK+RyOXCLiHwO3ELLv5nV0cBeevqtp9TYi0hH4G/AZe4T\nK5W6nAKsU9VlpMGo3iUEHA3cqapHAztpebdEHCJShDOq6I/zRtZRRM5KqVIJcKvZp4s7DhGZAlSp\n6qMp1iMf+AVwfXR3itSJJgQUqepxOIOuv6ZYnwgXAperal/gCmBWKpRw7eVcHHu5PXpbU37rKTP2\nIpKDo/jDqvpUqvSI4pvAKBFZDjwGfEdEHkyxTl/ijL7ecL//Dcf4p5rhwHJV3aiqNcDfce5fOrBW\nRHoAiEhPYF2K9QFARMbhuAjT4aFYivOg/o/7e+8N/J+IdEupVs7v/e8A7m8+LCIlqVUJgHNU9Un3\n899w8n61KFH28qEoe+npt56qaBwB7gfeV9XbU6FDQ1T1F6raR1UH4Ew2vqCq56RYpzXAFyJykNs1\nHHgvhSpFWAkcJyJ57t9yOM6kdjowDxjrfh4LpHwg4ab6vgr4garuSbU+qvqOqnZX1QHu7/1L4GhV\nTfWD8SngOwDubz5XVTemViUAVonIMPfzd3Am2VuMfdhLb791VW3xBnwLxy/+FrDMbSelQpdG9BsG\nzEu1Hq4uRwJvAP/BGfV0SrVOrl7TcCaK3sGZHMpJgQ6P4cwZVOEk3TsXKAaex/kPuRDonGKdzgM+\nxnlARn7rd6ZIp72R+9Rg+2dAcap1AnKAh9zf1P8BZWnymzoeWOraq1eBIS2sU0J76fW3bouqDMMw\nMoCUR+MYhmEYyceMvWEYRgZgxt4wDCMDMGNvGIaRAZixNwzDyADM2BuGYWQAZuwNwzAyADP2hmEY\nGYAZeyNjEZEnRWSpWxDiArfvfBH5SERed7OM/tHt7+oWkfm329IlF5BhNAlbQWtkLCJSpKqbRSQP\n+DdQDryCk0J2B/AC8JaqXioijwJ3qOorItIXWKCqh6ZMecPwSNIKjhtGK+AyEYkUfOgDnA1UquoW\nABF5AohOQvc1JycVAAUikq+qu1pSYcPwixl7IyMRkTLgROA4Vd0jIi8CHwLR5TGF+hzhAnxDVata\nVFHDCAjz2RuZSiGw2TX0h+DU9uwADBOnyHsIGB21/0Lg0sgXETmqRbU1jGZixt7IVBYAIRF5H5iB\nk7r2S+BmHP/9EmA5sM3d/1JgqIj8R0TeA8a3vMqG4R+boDWMKESkg6rudEf2fwfuV9WnU62XYTQX\nG9kbRizTRGQZTgGNz8zQG20FG9kbhmFkADayNwzDyADM2BuGYWQAZuwNwzAyADP2hmEYGYAZe8Mw\njAzAjL1hGEYG8P8BVNRlNW0XOLEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108a2e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#b) Apply your implementation to the entire dataset,\n",
    "#and record your alpha and beta values.\n",
    "alpha, beta = univarlinreg(age,FEV1)\n",
    "print alpha\n",
    "print beta\n",
    "#Plot your data points together with your estimated regression line.\n",
    "plt.scatter(age, FEV1, label = \"Data\")\n",
    "FEV1_reg = alpha + beta*age \n",
    "plt.plot(age, FEV1_reg, 'r', label=\"Regression line\")\n",
    "#Remember to label your axes and your plot.\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('FEV1')\n",
    "plt.title('Plot of FEV1 versus age')\n",
    "plt.legend(loc='lower right')\n",
    "#Describe what you see and discuss briefly how the variable age relates to FEV1 according to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 1) x: the independent variable, as a N dimensional vector as a numpy array\n",
    "#        2) y: the dependent variable, as a N dimensional vector as a numpy array\n",
    "#        3) alpha: the alpha parameter\n",
    "#        4) beta: the beta parameter\n",
    "#\n",
    "# output: 1) the root mean square error (rmse) \n",
    "\n",
    "def rmse(x, y, alpha, beta):\n",
    "    total_se = np.sum((y_i - (alpha + np.dot(beta,x_i)) )** 2 for x_i, y_i in zip(x, y))\n",
    "    rmse = np.sqrt( total_se / len(x) )\n",
    "    return rmse\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.572462844345\n"
     ]
    }
   ],
   "source": [
    "#d) Select 450 random subjects as a test set\n",
    "#and use only these subjects to build the regres- sion model.\n",
    "#Use the remaining 204 subjects to compute RMSE(alpha,beta) of your model.\n",
    "\n",
    "#separate out subjects for testing and for computing RMSE\n",
    "rand = np.random.permutation(len(age))\n",
    "test_indices = rand[:450]\n",
    "remain_indices = np.setdiff1d(rand, test_indices)\n",
    "\n",
    "FEV1_test = FEV1[test_indices]\n",
    "age_test = age[test_indices]\n",
    "alpha_test, beta_test = univarlinreg(age_test, FEV1_test)\n",
    "\n",
    "FEV1_remain = FEV1[remain_indices]\n",
    "age_remain = age[remain_indices]\n",
    "print rmse(age_remain,FEV1_remain,alpha_test,beta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 1) x: the independent variables (data matrix), as a N x M dimensional matrix as a numpy array\n",
    "#        2) y: the dependent variable, as a N dimensional vector as a numpy array\n",
    "#\n",
    "# output: 1) the regression coefficients as a (M+1) dimensional vector as a numpy array\n",
    "#\n",
    "# note: the regression coefficients should include the w_0 (the free parameter), thus having dimension (M+1).\n",
    "# note: The tested datamatrix is **NOT** extended with a column of 1's - if you prefer to do this, then you can do it inside the function by extending x.       \n",
    "def multivarlinreg(x, y):\n",
    "    r, c = x.shape\n",
    "    X = np.c_[np.ones(r), x ] #include column of 1s to data matrix\n",
    "    \n",
    "    w = np.dot( np.dot( ( np.linalg.inv(np.dot(np.transpose(X), X)) ), np.transpose(X) ), y)\n",
    "    return w\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 654)\n",
      "(654, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.64279712,  0.06579597,  0.10430932,  0.15469664, -0.08526862,\n",
       "        0.01781883])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b) Run your regression function on the entire dataset\n",
    "#and record your estimated parameters wi\n",
    "#What do they tell you about how lung function relates to the di↵erent input variables? Does smoking a↵ect FEV1?\n",
    "x = np.vstack( (age,height,gender,smoking_status,weight) )\n",
    "print x.shape\n",
    "x = np.transpose(x)\n",
    "print x.shape\n",
    "multivarlinreg( x, FEV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "(204,)\n",
      "(204,)\n",
      "0.398904737164\n",
      "0.395666890667\n"
     ]
    }
   ],
   "source": [
    "#c) Using the same randomly selected training\n",
    "#and test sets as in the previous exercise,\n",
    "#separate out subjects for testing and for computing RMSE\n",
    "#rand = np.random.permutation(len(age))\n",
    "test_indices = rand[:450] #train subjects\n",
    "remain_indices = np.setdiff1d(rand, test_indices) #test subjects\n",
    "\n",
    "FEV1_test = FEV1[test_indices]\n",
    "age_test = age[test_indices]\n",
    "alpha_test, beta_test = univarlinreg(age_test, FEV1_test)\n",
    "\n",
    "FEV1_remain = FEV1[remain_indices]\n",
    "age_remain = age[remain_indices]\n",
    "\n",
    "#estimate the free parameters wi for the model, and use these to predict output values on the test set.\n",
    "xtrain_rand = x[test_indices,:]\n",
    "FEV1_rand = FEV1[test_indices]\n",
    "w = multivarlinreg( xtrain_rand, FEV1_rand)\n",
    "print w.shape\n",
    "\n",
    "w_0 = w[0]\n",
    "w_not0 = w[1:]\n",
    "x_test = x[remain_indices,:]\n",
    "r, _ = x_test.shape\n",
    "X_test = np.c_[np.ones(r), x_test ]\n",
    "FEV1_test = FEV1[remain_indices]\n",
    "print FEV1_test.shape\n",
    "FEV1_pred = np.dot(w, np.transpose(X_test))\n",
    "print FEV1_pred.shape\n",
    "w_true = multivarlinreg( x_test, FEV1_test)\n",
    "w_0true = w_true[0]\n",
    "w_not0true = w_true[1:]\n",
    "#Run your RMSE function on the true and estimated output values for the test set. What do you see?\n",
    "print rmse(x_test, FEV1_test, w_0, w_not0) #estimated value\n",
    "print rmse(x_test, FEV1_test, w_0true, w_not0true) #true value\n",
    "\n",
    "#lower the error, the better => better fit of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implement gradient descent first to find w from training set\n",
    "#by iterating then changing the learning rate until the minimum is reached or something\n",
    "#how do I know when the minimum is reached?\n",
    "#use w to predict labels on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MY GRADIENT DESCENT\n",
    "# input:  1) train data (without labels) in the form of a N by d numpy array, where N is the number of training data points and d is the number of dimensions\n",
    "#         2) trainlabels: labels for training data in the form of a N by 1 numpy vector, where N is the number of training data points\n",
    "#         3) test: test data (without labels) in the form of a M by d numpy array, where M is the number of test data points and d is the number of dimensions\n",
    "#\n",
    "# output: 1) vector (numpy array) consisting of the predicted classes for the test data\n",
    "#         2) the beta parameter of the model as a (d+1) dimensional numpy array\n",
    "#\n",
    "# note: the labels should **not** be part of the train/test data matrices!\n",
    "# note: The datamatrices are **NOT** already extended with a column of 1's - you should do \n",
    "def logreg(train_data, train_labels, test_data):\n",
    "    #based on code from logistic regression lecture\n",
    "    #X = Xorig + column of 1s\n",
    "    def logistic(input):\n",
    "        out = np.exp(input)/(1+ np.exp(input))\n",
    "        return out\n",
    "    def logistic_insample(X, y, w):\n",
    "        N, num_feat = X.shape\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            E = E + (1/N)*np.log(1/logistic(y[n]*np.dot(w,X[n,:])))\n",
    "        return E\n",
    "    def logistic_gradient(X, y, w):\n",
    "        N, _ = X.shape\n",
    "        g = 0*w\n",
    "\n",
    "        for n in range(N):\n",
    "            g = g + ((-1/N)*y[n]*X[n,:])*logistic(-y[n]*np.dot(w,X[n,:]))\n",
    "        return g\n",
    "\n",
    "    def gradient_descent(Xorig, y, t_max, grad_thr):\n",
    "        #include column of 1s to train_data\n",
    "        N, d = Xorig.shape\n",
    "        X = np.c_[np.ones(N), Xorig]\n",
    "        dplus1 = d + 1 #number of weights needed\n",
    "\n",
    "        #initialise learning rate \n",
    "        eta = 0.1\n",
    "        #initialise with random sample from normal distribution with weights at time step 0\n",
    "        w = 0.1*np.random.randn(dplus1) #number of weights is equal to dimensions of training data + 1\n",
    "        #compute logistic log likelihood\n",
    "        E = logistic_insample(X, y, w)\n",
    "\n",
    "        #initialise number of iterations and set boundary conditions\n",
    "        t = 0\n",
    "        convergence = 0\n",
    "\n",
    "        #keep track of log likelihood values\n",
    "        E_in = []\n",
    "\n",
    "        #loop to iterate until the function converges?\n",
    "        while convergence == 0:\n",
    "            t = t + 1\n",
    "\n",
    "            #compute the gradient of the log-likelihood wrt the current w\n",
    "            g = logistic_gradient(X, y, w)\n",
    "\n",
    "            #move in opposite direction of gradient to minimise the log likelihood\n",
    "            v_t = -g\n",
    "\n",
    "            #take a step in new direction\n",
    "            eta_t = eta*np.linalg.norm(E_in)\n",
    "            w_new = w + eta_t*v_t\n",
    "\n",
    "            # Check for improvement\n",
    "            # Compute in-sample error for new w\n",
    "            E_t = logistic_insample(X, y, w_new)\n",
    "            \n",
    "            if E_t < E:\n",
    "                w = w_new\n",
    "                E = E_t\n",
    "                E_in.append(E)\n",
    "                eta *=1.05\n",
    "            else:\n",
    "                eta *= 0.95  \n",
    "\n",
    "            #not sure about this part\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            if g_norm < grad_thr:\n",
    "                convergence = 1\n",
    "            elif t > t_max:\n",
    "                convergence = 1\n",
    "            \n",
    "            return w\n",
    "        \n",
    "    w = gradient_descent(train_data, train_labels, 10000000, 0.000)\n",
    "    \n",
    "    #use optimised weights to predict labels for test set\n",
    "    def log_pred(Xorig, w):\n",
    "        N, _ = Xorig.shape\n",
    "        w_0 = w[0]\n",
    "        w_new = w[1:]\n",
    "        pred_classes = []\n",
    "        for i in range(0,N):\n",
    "            h_x = logistic(w[0] + np.dot((np.transpose(w_new)),Xorig[i,:]))\n",
    "            if h_x > 0.5:\n",
    "                pred_classes.append(1)\n",
    "            else:\n",
    "                pred_classes.append(0)\n",
    "        pred_classes = np.array(pred_classes) \n",
    "        return pred_classes\n",
    "        \n",
    "        \n",
    "    pred_labels = log_pred(test_data, w)\n",
    "    return pred_labels, w\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HER GRADIENT DESCENT\n",
    "# input:  1) train data (without labels) in the form of a N by d numpy array, where N is the number of training data points and d is the number of dimensions\n",
    "#         2) trainlabels: labels for training data in the form of a N by 1 numpy vector, where N is the number of training data points\n",
    "#         3) test: test data (without labels) in the form of a M by d numpy array, where M is the number of test data points and d is the number of dimensions\n",
    "#\n",
    "# output: 1) vector (numpy array) consisting of the predicted classes for the test data\n",
    "#\n",
    "# note: the labels should **not** be part of the train/test data matrices!\n",
    "def logreg(train_data, train_labels, test_data):\n",
    "    #turn train labels into +/- 1 labels\n",
    "    #y = (train_labels-0.5)*2\n",
    "    #based on code from logistic regression lecture\n",
    "    #X = Xorig + column of 1s\n",
    "    def logistic(input):\n",
    "        out = np.exp(input)/(1+ np.exp(input))\n",
    "        return out\n",
    "    def logistic_insample(X, y, w):\n",
    "        N, num_feat = X.shape\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            E = E + (1/N)*np.log(1/logistic(y[n]*np.dot(w,X[n,:])))\n",
    "        return E\n",
    "    def logistic_gradient(X, y, w):\n",
    "        N, _ = X.shape\n",
    "        g = 0*w\n",
    "\n",
    "        for n in range(N):\n",
    "            g = g + ((-1/N)*y[n]*X[n,:])*logistic(-y[n]*np.dot(w,X[n,:]))\n",
    "        return g\n",
    "\n",
    "    def gradient_descent(Xorig, y, t_max, grad_thr):\n",
    "        #include column of 1s to train_data\n",
    "        N, d = Xorig.shape\n",
    "        X = np.c_[np.ones(N), Xorig]\n",
    "        dplus1 = d + 1 #number of weights needed\n",
    "\n",
    "        #initialise learning rate \n",
    "        eta = 0.01\n",
    "        #initialise with random sample from normal distribution with weights at time step 0\n",
    "        w = 0.1*np.random.randn(dplus1) #number of weights is equal to dimensions of training data + 1\n",
    "        #compute logistic log likelihood\n",
    "        E = logistic_insample(X, y, w)\n",
    "\n",
    "        #initialise number of iterations and set boundary conditions\n",
    "        t = 0\n",
    "        convergence = 0\n",
    "\n",
    "        #keep track of log likelihood values\n",
    "        E_in = []\n",
    "\n",
    "        #loop to iterate until the function converges?\n",
    "        while convergence == 0:\n",
    "            t = t + 1\n",
    "\n",
    "            #compute the gradient of the log-likelihood wrt the current w\n",
    "            g = logistic_gradient(X, y, w)\n",
    "\n",
    "            #move in opposite direction of gradient to minimise the log likelihood\n",
    "            v_t = -g\n",
    "\n",
    "            #take a step in new direction\n",
    "            eta_t = eta*np.linalg.norm(E_in)\n",
    "            w_new = w + eta_t*v_t\n",
    "\n",
    "            # Check for improvement\n",
    "            # Compute in-sample error for new w\n",
    "            E_t = logistic_insample(X, y, w_new)\n",
    "            \n",
    "            if E_t < E:\n",
    "                w = w_new\n",
    "                E = E_t\n",
    "                E_in.append(E)\n",
    "                eta *=1.1\n",
    "            else:\n",
    "                eta *= 0.9   \n",
    "\n",
    "            #not sure about this part\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            if g_norm < grad_thr:\n",
    "                convergence = 1\n",
    "            elif t > t_max:\n",
    "                convergence = 1\n",
    "            \n",
    "            return w\n",
    "        \n",
    "    w = gradient_descent(train_data, train_labels, 20000, 0.000)\n",
    "    \n",
    "    #use optimised weights to predict labels for test set\n",
    "    def log_pred(Xorig, w):\n",
    "        Norig, _ = Xorig.shape\n",
    "        X = np.c_[np.ones(Norig), Xorig]\n",
    "        N, _ = X.shape\n",
    "        P = np.zeros(N)\n",
    "        for i in range(0,N):\n",
    "            arg = np.exp(np.dot(w,X[i,:]))\n",
    "            prob_i = arg/(1+arg)\n",
    "            P[i] = prob_i\n",
    "            \n",
    "        Pthresh=np.round(P)   #0/1 class labels\n",
    "        pred_classes=(Pthresh-0.5)*2\n",
    "        return pred_classes\n",
    "        \n",
    "        \n",
    "    return log_pred(test_data, w), w\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11227949  0.03028052  0.00708593  0.00730414 -0.14223258  0.15200695\n",
      " -0.0291394  -0.01330903 -0.01730696 -0.17616517 -0.00876731  0.13668794\n",
      "  0.11253141 -0.03589956  0.12206081 -0.13394955  0.04283734 -0.01234631\n",
      "  0.14143772 -0.01240507]\n",
      "0.255230125523\n",
      "0.389121338912\n"
     ]
    }
   ],
   "source": [
    "#b) Train the logistic regression on the training set\n",
    "#and run it on the test set.\n",
    "#Threshold the returned probabilities at 0.5 to obtain a binary classification,\n",
    "#and compute the test error in percentage.\n",
    "train_data = np.loadtxt('DD/DD_train.txt')\n",
    "train_labels = np.loadtxt('DD/DD_train_labels.txt')\n",
    "test_data = np.loadtxt('DD/DD_test.txt')\n",
    "test_labels = np.loadtxt('DD/DD_test_labels.txt')\n",
    "\n",
    "pred_labels, w = logreg(train_data, train_labels, test_data)\n",
    "print w\n",
    "compare_labels = test_labels - pred_labels\n",
    "wrong_labels = filter(lambda a: a != 0, list(compare_labels))\n",
    "\n",
    "\n",
    "err_best = 186/478\n",
    "err = len(wrong_labels)/len(test_labels)\n",
    "print err\n",
    "print err_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine data\n",
      "0.0003 1e-07\n",
      "train accuracy from implementation 755\n",
      "test accuracy from implementation 427\n",
      "DD data\n",
      "0.0003 1e-07\n",
      "train accuracy from implementation 491\n",
      "test accuracy from implementation 333\n",
      "333\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11093ec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn import datasets\n",
    "\n",
    "def logistic_func(theta, x):\n",
    "    return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "def log_gradient(theta, x, y):\n",
    "    first_calc = logistic_func(theta, x) - np.squeeze(y)\n",
    "    final_calc = first_calc.T.dot(x)\n",
    "    return final_calc\n",
    "def cost_func(theta, x, y):\n",
    "    log_func_v = logistic_func(theta,x)\n",
    "    y = np.squeeze(y)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "\n",
    "    final = -step1 - step2\n",
    "    return np.mean(final)\n",
    "def grad_desc(theta_values, X, y, lr=3e-4, converge_change=1e-7):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    #setup cost iter\n",
    "    cost_iter = []\n",
    "    cost = cost_func(theta_values, X, y)\n",
    "    cost_iter.append([0, cost])\n",
    "    change_cost = 1\n",
    "    i = 1\n",
    "    while(change_cost > converge_change):\n",
    "        old_cost = cost\n",
    "        theta_values = theta_values - (lr * log_gradient(theta_values, X, y))\n",
    "        cost = cost_func(theta_values, X, y)\n",
    "        cost_iter.append([i, cost])\n",
    "        change_cost = old_cost - cost\n",
    "        i+=1\n",
    "    print lr, converge_change\n",
    "    return theta_values, np.array(cost_iter)\n",
    "def pred_values(theta, X, hard=True):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    \n",
    "    pred_prob = logistic_func(theta, X)\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    return pred_prob\n",
    "\n",
    "#This code is based http://stackoverflow.com/questions/26248654/numpy-return-0-with-divide-by-zero[1]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def cent(data):\n",
    "    \"\"\"Rescale features of data to have properties of a standard normal distribution with zero mean and variance of 1.\"\"\"\n",
    "    \n",
    "    np.random.seed(0) #to remove randomisation effects\n",
    "    \n",
    "    mean_cols = np.mean(data, axis=0)\n",
    "    var_cols = np.sqrt(np.var(data, axis = 0))\n",
    "    r, _ = data.shape\n",
    "    mean_matrix = np.array([mean_cols]*r)\n",
    "    var_matrix = np.array([var_cols]*r)\n",
    "    #The following part is based on [1]\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cent_data = np.true_divide((data - mean_matrix),var_matrix)\n",
    "        cent_data[cent_data == np.inf] = 0\n",
    "        cent_data = np.nan_to_num(cent_data)\n",
    "    \n",
    "    return cent_data\n",
    "\n",
    "\n",
    "print 'wine data'\n",
    "X = np.loadtxt('exam/redwinedata/redwine_train.txt')\n",
    "y = np.loadtxt('exam/redwinedata/redwine_trainlabels.txt')\n",
    "#print y\n",
    "test = np.loadtxt('exam/redwinedata/redwine_test.txt')\n",
    "testlabels = np.loadtxt('exam/redwinedata/redwine_testlabels.txt')\n",
    "\n",
    "X = cent(X)\n",
    "test = cent(test)\n",
    "\n",
    "r, d = X.shape\n",
    "#y_flip = np.logical_not(y) #flip Setosa to be 1 and Versicolor to zero to be consistent\n",
    "#print y_flip\n",
    "betas = np.zeros(d)\n",
    "fitted_values, cost_iter = grad_desc(betas, X, y)\n",
    "#print 'w from implementation', (fitted_values)\n",
    "\n",
    "\n",
    "predicted_y = pred_values(fitted_values, X)\n",
    "print 'train accuracy from implementation', np.sum(y == predicted_y)\n",
    "\n",
    "\n",
    "\n",
    "predicted_test = pred_values(fitted_values, test)\n",
    "print 'test accuracy from implementation', np.sum(testlabels == predicted_test)\n",
    "\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "pred = logreg.predict(test)\n",
    "#print 'train accuracy from built in', sum(y == logreg.predict(X))\n",
    "#print 'test accuracy from built in', sum(testlabels == logreg.predict(test))\n",
    "w = logreg.coef_\n",
    "#print w[0]\n",
    "\n",
    "\n",
    "plt.plot(cost_iter[:,0], cost_iter[:,1])\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "#lr=0.0003, converge_change=.000000001 497\n",
    "\n",
    "train_data = np.loadtxt('DD/DD_train.txt')\n",
    "train_labels = np.loadtxt('DD/DD_train_labels.txt')\n",
    "test_data = np.loadtxt('DD/DD_test.txt')\n",
    "test_labels = np.loadtxt('DD/DD_test_labels.txt')\n",
    "\n",
    "train_data = cent(train_data)\n",
    "test_data = cent(test_data)\n",
    "\n",
    "print 'DD data'\n",
    "r, d = train_data.shape\n",
    " #flip Setosa to be 1 and Versicolor to zero to be consistent\n",
    "#print y_flip\n",
    "betas = np.zeros(d)\n",
    "fitted_values1, cost_iter = grad_desc(betas, train_data, train_labels)\n",
    "#print 'w from implementation', (fitted_values1)\n",
    "\n",
    "\n",
    "predicted_y1 = pred_values(fitted_values1, train_data)\n",
    "print 'train accuracy from implementation', np.sum(train_labels == predicted_y1)\n",
    "\n",
    "predicted_test1 = pred_values(fitted_values1, test_data)\n",
    "print 'test accuracy from implementation', np.sum(test_labels == predicted_test1)\n",
    "\n",
    "compare_labels = predicted_test1 - test_labels\n",
    "#print compare_labels\n",
    "wrong_labels = filter(lambda a: a != 0, list(compare_labels))\n",
    "#print wrong_labels\n",
    "#print no_wrong_labels\n",
    "print len(predicted_test1) - len(wrong_labels)\n",
    "\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(train_data, train_labels)\n",
    "#print 'train accuracy from built in', sum(train_labels == logreg.predict(train_data))\n",
    "#print 'test accuracy from built in', sum(test_labels == logreg.predict(test_data))\n",
    "w = logreg.coef_\n",
    "#print w[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "train accuracy from implementation 267\n",
      "test accuracy from implementation 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:142: DeprecationWarning: elementwise comparison failed; this will raise the error in the future.\n"
     ]
    }
   ],
   "source": [
    "train_data = np.loadtxt('DD/DD_train.txt')\n",
    "train_labels = np.loadtxt('DD/DD_train_labels.txt')\n",
    "test_data = np.loadtxt('DD/DD_test.txt')\n",
    "test_labels = np.loadtxt('DD/DD_test_labels.txt')\n",
    "\n",
    "def cent(data):\n",
    "    \"\"\"Rescale features of data to have properties of a standard normal distribution with zero mean and variance of 1.\"\"\"\n",
    "    \n",
    "    np.random.seed(0) #to remove randomisation effects\n",
    "    \n",
    "    mean_cols = np.mean(data, axis=0)\n",
    "    var_cols = np.sqrt(np.var(data, axis = 0))\n",
    "    r, _ = data.shape\n",
    "    mean_matrix = np.array([mean_cols]*r)\n",
    "    var_matrix = np.array([var_cols]*r)\n",
    "    #The following part is based on [1]\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cent_data = np.true_divide((data - mean_matrix),var_matrix)\n",
    "        cent_data[cent_data == np.inf] = 0\n",
    "        cent_data = np.nan_to_num(cent_data)\n",
    "    \n",
    "    return cent_data\n",
    "\n",
    "train_data = cent(train_data)\n",
    "test_data = cent(test_data)\n",
    "\n",
    "#WITH BUILT IN FUNCTION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# input:  1) train data (without labels) in the form of a N by d numpy array, where N is the number of training data points and d is the number of dimensions\n",
    "#         2) trainlabels: labels for training data in the form of a N by 1 numpy vector, where N is the number of training data points\n",
    "#         3) test: test data (without labels) in the form of a M by d numpy array, where M is the number of test data points and d is the number of dimensions\n",
    "#\n",
    "# output: 1) vector (numpy array) consisting of the predicted classes for the test data\n",
    "#\n",
    "# note: the labels should **not** be part of the train/test data matrices!\n",
    "def logreg(train_data, train_labels, test_data):\n",
    "    #based on code from logistic regression lecture\n",
    "    #X = Xorig + column of 1s\n",
    "    def logistic(input):\n",
    "        out = np.exp(input)/(1+ np.exp(input))\n",
    "        return out\n",
    "    def logistic_insample(X, y, w):\n",
    "        N, num_feat = X.shape\n",
    "        E = 0\n",
    "        for n in range(N):\n",
    "            E = E + (1/N)*np.log(1/logistic(y[n]*np.dot(w,X[n,:])))\n",
    "        return E\n",
    "    def logistic_gradient(X, y, w):\n",
    "        N, _ = X.shape\n",
    "        g = 0*w\n",
    "\n",
    "        for n in range(N):\n",
    "            g = g + ((-1/N)*y[n]*X[n,:])*logistic(-y[n]*np.dot(w,X[n,:]))\n",
    "        return g\n",
    "\n",
    "    def gradient_descent(Xorig, y, t_max, grad_thr):\n",
    "        #include column of 1s to train_data\n",
    "        N, d = Xorig.shape\n",
    "        X = np.c_[np.ones(N), Xorig]\n",
    "        dplus1 = d + 1 #number of weights needed\n",
    "\n",
    "        #initialise learning rate \n",
    "        eta = 1e-3\n",
    "        #initialise with random sample from normal distribution with weights at time step 0\n",
    "        w = 0.1*np.random.randn(dplus1) #number of weights is equal to dimensions of training data + 1\n",
    "        #compute logistic log likelihood\n",
    "        E = logistic_insample(X, y, w)\n",
    "\n",
    "        #initialise number of iterations and set boundary conditions\n",
    "        t = 0\n",
    "        convergence = 0\n",
    "\n",
    "        #keep track of log likelihood values\n",
    "        E_in = []\n",
    "\n",
    "        #loop to iterate until the function converges?\n",
    "        while convergence == 0:\n",
    "            t = t + 1\n",
    "\n",
    "            #compute the gradient of the log-likelihood wrt the current w\n",
    "            g = logistic_gradient(X, y, w)\n",
    "\n",
    "            #move in opposite direction of gradient to minimise the log likelihood\n",
    "            v_t = -g\n",
    "\n",
    "            #take a step in new direction\n",
    "            eta_t = eta*np.linalg.norm(E_in)\n",
    "            w_new = w + eta_t*v_t\n",
    "\n",
    "            # Check for improvement\n",
    "            # Compute in-sample error for new w\n",
    "            E_t = logistic_insample(X, y, w_new)\n",
    "            \n",
    "            if E_t < E:\n",
    "                w = w_new\n",
    "                E = E_t\n",
    "                E_in.append(E)\n",
    "                eta *=1.01\n",
    "            else:\n",
    "                eta *= 0.99\n",
    "\n",
    "            #not sure about this part\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            if g_norm < grad_thr:\n",
    "                convergence = 1\n",
    "                \n",
    "            if t > t_max:\n",
    "                convergence = 1\n",
    "            \n",
    "            return w\n",
    "        \n",
    "    w = gradient_descent(train_data, train_labels, 1e7, 0.000)\n",
    "    print len(w)\n",
    "    #use optimised weights to predict labels for test set\n",
    "    def log_pred(Xorig, w, hard = True):\n",
    "        N, _ = Xorig.shape\n",
    "        w_0 = w[0]\n",
    "        w_new = w[1:]\n",
    "        \n",
    "        pred_classes = []\n",
    "        for i in range(0,N):\n",
    "            h_x = logistic(w[0] + np.dot((np.transpose(w_new)),Xorig[i,:]))\n",
    "            \n",
    "        pred_prob = logistic(w_new.dot (Xorig.T))\n",
    "        pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "        if hard:\n",
    "            return pred_value\n",
    "        return pred_prob\n",
    "    pred_labels = log_pred(train_data, w)\n",
    "    #using code from http://www.dummies.com/how-to/content/using-logistic-regression-in-python-for-data-scien.html\n",
    "    logistic = LogisticRegression()\n",
    "    logistic.fit(train_data, train_labels)\n",
    "    pred_labels_builtin = logistic.predict(test_data)\n",
    "       \n",
    "    return pred_labels\n",
    "    \n",
    "    pass\n",
    "pred_train = logreg(train_data, train_labels, train_data)\n",
    "pred_test = logreg(train_data, train_labels, test_data)\n",
    "print 'train accuracy from implementation', np.sum(train_labels == pred_train)\n",
    "print 'test accuracy from implementation', np.sum(test_labels == pred_test)\n",
    "#[ 0.20863823 -0.00998332 -0.02211062 -0.01353298  0.01095668 -0.00755535\n",
    " #-0.02487498 -0.03687053  0.01177903 -0.00086013 -0.0179381   0.29183236\n",
    "#-0.06018673  0.29287009 -0.55967055 -0.54627027 -0.12652795  0.\n",
    " #-0.04217598]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(input):\n",
    "    out = np.exp(input)/(1+ np.exp(input))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 19)\n",
      "[ 0.20863823 -0.00998332 -0.02211062 -0.01353298  0.01095668 -0.00755535\n",
      " -0.02487498 -0.03687053  0.01177903 -0.00086013 -0.0179381   0.29183236\n",
      " -0.06018673  0.29287009 -0.55967055 -0.54627027 -0.12652795  0.\n",
      " -0.04217598]\n",
      "(1, 19)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "train_data = np.loadtxt('DD/DD_train.txt')\n",
    "train_labels = np.loadtxt('DD/DD_train_labels.txt')\n",
    "test_data = np.loadtxt('DD/DD_test.txt')\n",
    "test_labels = np.loadtxt('DD/DD_test_labels.txt')\n",
    "print train_data.shape\n",
    "\n",
    "logR = LogisticRegression()\n",
    "logR.fit(train_data, train_labels)\n",
    "w = logR.coef_\n",
    "print w[0]\n",
    "pred_log = logR.predict(test_data)\n",
    "print w.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#c) Run your implemented k-NN algorithm from Assignment 2 on the same dataset,\n",
    "#se- lecting an optimal k by using cross validation on the training set as in Assignment 2.\n",
    "#Which classifier works best on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from collections import Counter\n",
    "\n",
    "# input: train: 1) train data (without labels) in the form of a N by d numpy array, where N is the number of training data points and d is the number of dimensions\n",
    "#               2) test: test data (without labels) in the form of a M by d numpy array, where M is the number of test data points and d is the number of dimensions\n",
    "#               3) trainlabels: labels for training data in the form of a N by 1 numpy vector, where N is the number of training data points\n",
    "#               4) k: paramater k\n",
    "# output:1) distance matrix (numpy array) between test and training samples \n",
    "#        2) vector (numpy array) consisting of the predicted classes for the test data\n",
    "#\n",
    "# note: the labels should **not** be part of the train/test data matrices!\n",
    "def knn(train, test, trainlabels, k):\n",
    "    samples = ([train], trainlabels)\n",
    "    \n",
    "    #from textbook\n",
    "    def majority_vote(labels):\n",
    "        \"\"\"assumes that labels are ordered from nearest to farthest\"\"\"\n",
    "        vote_counts = Counter(labels)\n",
    "        winner, winner_count = vote_counts.most_common(1)[0]\n",
    "        num_winners = len([count\n",
    "                            for count in vote_counts.values()\n",
    "                            if count == winner_count])\n",
    "        if num_winners == 1:\n",
    "            return winner # unique winner, so return it\n",
    "        else:\n",
    "            return majority_vote(labels[:-1]) # try again without the far    \n",
    "    #----------------------\n",
    "    \n",
    "    #compute Euclidean distances of test samples from each training samples\n",
    "    def distances(x, y):\n",
    "        dist = 0\n",
    "        for i in range(len(x)):\n",
    "            squared_dist = (x[i] - y[i])**2\n",
    "            dist += squared_dist\n",
    "        return math.sqrt(dist)\n",
    "    \n",
    "    dist = []\n",
    "    for a in test:\n",
    "        row_dist = []\n",
    "        for b in train:\n",
    "            row_dist.append(distances(a,b))\n",
    "        dist.append(row_dist)\n",
    "    dist = np.array(dist)\n",
    "    dist = dist.T\n",
    "    r, c = dist.shape\n",
    "    \n",
    "\n",
    "    #sort distance in order for each test sample (each column in distance matrix)\n",
    "    predicted_labels_list = []\n",
    "    for i in range(0,c):\n",
    "        test_i = dist[:,i]\n",
    "        test_i_distance_sorted = np.argsort(test_i) #indices of distances sorted in ascending order\n",
    "        #cut this off at the k-th nearest label to find the corresponding indices in trainlabels\n",
    "        k_nearest_labels = trainlabels[test_i_distance_sorted[:k]]\n",
    "        #use this to vote on the label of the k nearest neighbours of test sample\n",
    "        predicted_labels_list.append(majority_vote(k_nearest_labels))\n",
    "    predicted_labels = np.array(predicted_labels_list)\n",
    "    dist = dist.T\n",
    "    return predicted_labels\n",
    "\n",
    "# input:  1) training data in the form of a N by d numpy array, where N is the number of training data points and d is the number of dimensions\n",
    "#         2) training labels in the form of a N by 1 numpy vector, where N is the number of training data points\n",
    "#         3) a random permutation of entries as a numpy array, e.g. np.random.permutation(len(trainlabels))\n",
    "# output: 1) the optimal k\n",
    "#         2) an error matrix (numpy array) of size (5,13) where column i consists of the accuracy for the 5 folds for k=i\n",
    "#\n",
    "# The random-permuted vector rand_perm should be used for generating 5 folds, where the first fold consists of the first N/5 elements from rand_perm, rounded up to the nearest integer; the second fold consists of the next N/5 elements, etc, and the fifth fold consists of the remaining elements\n",
    "# note: to create the folds consider: KFold(len(trainlabels), n_folds=5) from sklearn.cross_validation (http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html)\n",
    "# note: once you have the folds use the rand_perm vector to get the random indices in the training data and labels\n",
    "def cv(train, trainlabels, rand_perm):\n",
    "    \n",
    "    #based on http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
    "    kf = KFold(len(trainlabels), n_folds=5)\n",
    "    X = train\n",
    "    y = trainlabels\n",
    "    \n",
    "    error_lists = []\n",
    "    mylist = list(kf)\n",
    "    for i in range (0, 5):\n",
    "        train_index, test_index = mylist[i]\n",
    "        #X_train, X_test = X[train_index], X[test_index]\n",
    "        #y_train, y_test = y[train_index], y[test_index]\n",
    "        test_indices = rand_perm[test_index]\n",
    "        train_indices = rand_perm[train_index]\n",
    "        \n",
    "        #pick out rows corresponding to those indices\n",
    "        train_data = train[train_indices,:] \n",
    "        test_data = train[test_indices,:]\n",
    "        fold_error = []\n",
    "        for j in xrange(1,26,2):\n",
    "            predicted_labels = knn(train_data, test_data, trainlabels, j)\n",
    "            #print predicted_labels\n",
    "            test_labels = trainlabels[test_indices]\n",
    "            #print test_labels\n",
    "            compare_labels = predicted_labels - test_labels\n",
    "            #print compare_labels\n",
    "            wrong_labels = filter(lambda a: a != 0, list(compare_labels))\n",
    "            #print wrong_labels\n",
    "            no_wrong_labels = len(wrong_labels)\n",
    "            #print no_wrong_labels\n",
    "            error = no_wrong_labels/len(test_labels)\n",
    "            #print error\n",
    "            fold_error.append(error)\n",
    "        error_lists.append(fold_error)\n",
    "        error_matrix = np.array(error_lists)\n",
    "    \n",
    "    r, c = error_matrix.shape\n",
    "    k_error_list = []\n",
    "    for x in range(0,c):\n",
    "        k_error = error_matrix[:,x]\n",
    "        k_error_avg = np.mean(k_error)\n",
    "        k_error_list.append(k_error_avg)\n",
    "    k_best_index = np.argmin(k_error_list)\n",
    "    #print k_error_list\n",
    "    k_values = np.arange(1,26,2)\n",
    "    #print k_values\n",
    "    #print k_best_index\n",
    "    k_best = k_values[k_best_index]\n",
    "    #print k_best\n",
    "    \n",
    "    return k_best\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240585774059\n"
     ]
    }
   ],
   "source": [
    "rand_perm = np.random.permutation(len(train_labels))\n",
    "\n",
    "k_best = cv(train_data, train_labels, rand_perm)\n",
    "predicted_labels = knn(train_data, test_data, train_labels, k_best)\n",
    "\n",
    "#print predicted_labels\n",
    "#test = np.loadtxt('data_handin2/parkinsonsTest.dt')\n",
    "#testlabels = test[:,22]\n",
    "#print test_labels\n",
    "compare_labels = predicted_labels - test_labels\n",
    "#print compare_labels\n",
    "wrong_labels = filter(lambda a: a != 0, list(compare_labels))\n",
    "#print wrong_labels\n",
    "#print no_wrong_labels\n",
    "print len(wrong_labels)/len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine data\n",
      "3e-07 1e-05\n",
      "train accuracy from implementation 735\n",
      "test accuracy from implementation 418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11c024890>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEPCAYAAACDTflkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJ5+/+ZEgRvoPQtUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf77110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn import datasets\n",
    "\n",
    "def logistic_func(theta, x):\n",
    "    return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "def log_gradient(theta, x, y):\n",
    "    first_calc = logistic_func(theta, x) - np.squeeze(y)\n",
    "    final_calc = first_calc.T.dot(x)\n",
    "    return final_calc\n",
    "def cost_func(theta, x, y):\n",
    "    log_func_v = logistic_func(theta,x)\n",
    "    y = np.squeeze(y)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "\n",
    "    final = -step1 - step2\n",
    "    return np.mean(final)\n",
    "def grad_desc(theta_values, X, y, lr=3e-4, converge_change=1e-5):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    #setup cost iter\n",
    "    cost_iter = [] #E_t\n",
    "    cost = cost_func(theta_values, X, y)\n",
    "    cost_iter.append([0, cost]) #E_t\n",
    "    change_cost = 1 \n",
    "    i = 1 #t \n",
    "    while(change_cost > converge_change):\n",
    "        old_cost = cost #\n",
    "        theta_values = theta_values - (lr * log_gradient(theta_values, X, y))\n",
    "        cost = cost_func(theta_values, X, y)\n",
    "        cost_iter.append([i, cost])\n",
    "        change_cost = old_cost - cost\n",
    "        i+=1\n",
    "    print lr, converge_change\n",
    "    return theta_values, np.array(cost_iter)\n",
    "def pred_values(theta, X, hard=True):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    \n",
    "    pred_prob = logistic_func(theta, X)\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    return pred_prob\n",
    "\n",
    "#This code is based http://stackoverflow.com/questions/26248654/numpy-return-0-with-divide-by-zero[1]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def cent(data):\n",
    "    \"\"\"Rescale features of data to have properties of a standard normal distribution with zero mean and variance of 1.\"\"\"\n",
    "    \n",
    "    np.random.seed(0) #to remove randomisation effects\n",
    "    \n",
    "    mean_cols = np.mean(data, axis=0)\n",
    "    var_cols = np.sqrt(np.var(data, axis = 0))\n",
    "    r, _ = data.shape\n",
    "    mean_matrix = np.array([mean_cols]*r)\n",
    "    var_matrix = np.array([var_cols]*r)\n",
    "    #The following part is based on [1]\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cent_data = np.true_divide((data - mean_matrix),var_matrix)\n",
    "        cent_data[cent_data == np.inf] = 0\n",
    "        cent_data = np.nan_to_num(cent_data)\n",
    "    \n",
    "    return cent_data\n",
    "\n",
    "\n",
    "print 'wine data'\n",
    "X = np.loadtxt('exam/redwinedata/redwine_train.txt')\n",
    "y = np.loadtxt('exam/redwinedata/redwine_trainlabels.txt')\n",
    "#print y\n",
    "test = np.loadtxt('exam/redwinedata/redwine_test.txt')\n",
    "testlabels = np.loadtxt('exam/redwinedata/redwine_testlabels.txt')\n",
    "\n",
    "X = cent(X)\n",
    "test = cent(test)\n",
    "\n",
    "r, d = X.shape\n",
    "#y_flip = np.logical_not(y) #flip Setosa to be 1 and Versicolor to zero to be consistent\n",
    "#print y_flip\n",
    "betas = np.zeros(d)\n",
    "fitted_values, cost_iter = grad_desc(betas, X, y)\n",
    "#print 'w from implementation', (fitted_values)\n",
    "\n",
    "\n",
    "predicted_y = pred_values(fitted_values, X)\n",
    "print 'train accuracy from implementation', np.sum(y == predicted_y)\n",
    "\n",
    "\n",
    "\n",
    "predicted_test = pred_values(fitted_values, test)\n",
    "print 'test accuracy from implementation', np.sum(testlabels == predicted_test)\n",
    "\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "pred = logreg.predict(test)\n",
    "#print 'train accuracy from built in', sum(y == logreg.predict(X))\n",
    "#print 'test accuracy from built in', sum(testlabels == logreg.predict(test))\n",
    "w = logreg.coef_\n",
    "#print w[0]\n",
    "\n",
    "\n",
    "plt.plot(cost_iter[:,0], cost_iter[:,1])\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "#lr=0.0003, converge_change=.000000001 497\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
