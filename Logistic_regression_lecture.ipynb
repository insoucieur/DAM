{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(input):\n",
    "    out = np.exp(input)/(1+ np.exp(input))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1069ebe50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEZCAYAAACXRVJOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCGZmBsD/ACXIRu/TEOjrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104fa8210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10,10,num=1000)\n",
    "plt.plot(x, logistic(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Plot of logistic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('admission_dataset.txt')\n",
    "data_matrix = data[:,[0,1]]\n",
    "admission_labels = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_insample(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    E = 0\n",
    "    for n in range(N):\n",
    "        E = E + np.log(1+ np.exp(-y[n]*np.dot((np.transpose(w)), X[n,:])))\n",
    "    return E/N\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of checking our logistic log likelihood function logistic_insample, let's simulate a starting value of $w$ and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940931285174\n"
     ]
    }
   ],
   "source": [
    "N, num_feat = data_matrix.shape\n",
    "#Append a column of ones to the data matrix\n",
    "onevec = np.ones((N,1))\n",
    "X = np.concatenate((onevec, data_matrix), axis = 1)\n",
    "# Let's turn the admission labels into +/- 1 labels\n",
    "y = (admission_labels-0.5)*2\n",
    "\n",
    "np.random.seed(0)\n",
    "w = 0.1*np.random.randn(num_feat + 1)\n",
    "L = logistic_insample(X, y, w)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.940931285174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_gradient(X, y, w):\n",
    "    N, _ = X.shape\n",
    "    g = 0*w\n",
    "    \n",
    "    for n in range(N):\n",
    "        g = g + (-y[n]*X[n,:]*logistic(-y[n]*np.dot((np.transpose(w)),X[n,:])))\n",
    "    return g/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for checking the gradient function, let's evaluate it on our simulated $w$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3902137   1.29587263  2.22260581]\n"
     ]
    }
   ],
   "source": [
    "G = logistic_gradient(X,y,w)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine data\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn import datasets\n",
    "\n",
    "def logistic_func(theta, x):\n",
    "    return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "def log_gradient(theta, x, y):\n",
    "    first_calc = logistic_func(theta, x) - np.squeeze(y)\n",
    "    final_calc = first_calc.T.dot(x)\n",
    "    return final_calc\n",
    "def cost_func(theta, x, y):\n",
    "    log_func_v = logistic_func(theta,x)\n",
    "    y = np.squeeze(y)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "\n",
    "    final = -step1 - step2\n",
    "    return np.mean(final)\n",
    "def grad_desc(theta_values, X, y, lr=8e-7, converge_change=1e-10, lr_change = 1.4):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    #setup cost iter\n",
    "    cost_iter = [] #E_in\n",
    "    cost = cost_func(theta_values, X, y)\n",
    "    cost_iter.append([0, cost]) #E_in\n",
    "    change_cost = 1 \n",
    "    i = 1 #t \n",
    "    convergence = 0\n",
    "    while (change_cost > converge_change): #converges when there is no change in (E - E_t)\n",
    "        old_cost = cost #E = E_t\n",
    "        theta_values = theta_values - (lr * log_gradient(theta_values, X, y))\n",
    "        #w = w - (lr*g)\n",
    "        cost = cost_func(theta_values, X, y) #E_t\n",
    "        cost_iter.append([i, cost]) #append E_t to E_in\n",
    "        change_cost = old_cost - cost\n",
    "        \n",
    "         #E - E_t\n",
    "        #should consider changing learning rate here so if E - E_t > 0 then increase the learning rate\n",
    "        #otherwise slow it down\n",
    "        if old_cost > cost:\n",
    "            lr *=lr_change\n",
    "        else:\n",
    "            lr*=0.9\n",
    "        #if change_cost < converge_change:\n",
    "            #convergence = 1\n",
    "        g = log_gradient((theta_values), X, y)\n",
    "        #if np.linalg.norm(g) < 0:\n",
    "            #convergence = 1\n",
    "        \"\"\"elif i > max_i:\n",
    "            convergence = 1\"\"\"\n",
    "        i +=1\n",
    "        #t = t + 1\n",
    "    print lr, converge_change\n",
    "    return theta_values, np.array(cost_iter)\n",
    "def pred_values(theta, X, hard=True):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    \n",
    "    pred_prob = logistic_func(theta, X)\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    return pred_value\n",
    "\n",
    "#for i in range (0,10):\n",
    "    #w, _ = grad_desc(theta_values, X, y, lr=i*1e-7, converge_change=1e-10, max_i = 1e5)\n",
    "    #pred_label = \n",
    "#This code is based http://stackoverflow.com/questions/26248654/numpy-return-0-with-divide-by-zero[1]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def cent(data):\n",
    "    \"\"\"Rescale features of data to have properties of a standard normal distribution with zero mean and variance of 1.\"\"\"\n",
    "    \n",
    "    np.random.seed(0) #to remove randomisation effects\n",
    "    \n",
    "    mean_cols = np.mean(data, axis=0)\n",
    "    var_cols = np.sqrt(np.var(data, axis = 0))\n",
    "    r, _ = data.shape\n",
    "    mean_matrix = np.array([mean_cols]*r)\n",
    "    var_matrix = np.array([var_cols]*r)\n",
    "    #The following part is based on [1]\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cent_data = np.true_divide((data - mean_matrix),var_matrix)\n",
    "        cent_data[cent_data == np.inf] = 0\n",
    "        cent_data = np.nan_to_num(cent_data)\n",
    "    \n",
    "    return cent_data\n",
    "\n",
    "\n",
    "print 'wine data'\n",
    "X = np.loadtxt('exam/redwinedata/redwine_train.txt')\n",
    "y = np.loadtxt('exam/redwinedata/redwine_trainlabels.txt')\n",
    "#print y\n",
    "test = np.loadtxt('exam/redwinedata/redwine_test.txt')\n",
    "testlabels = np.loadtxt('exam/redwinedata/redwine_testlabels.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 0.1\n",
      "[-0.00944149 -0.00011593]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "np.random.seed(0)\n",
    "from sklearn import datasets\n",
    "\n",
    "def logistic(input):\n",
    "    out = np.exp(input)/(1+ np.exp(input))\n",
    "    return out\n",
    "def logistic_insample(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    E = 0\n",
    "    for n in range(N):\n",
    "        E = E + (1/N)*np.log(1/logistic(y[n]*np.dot(w,X[n,:])))\n",
    "    return E\n",
    "def logistic_gradient(X, y, w):\n",
    "    N, _ = X.shape\n",
    "    g = 0*w\n",
    "\n",
    "    for n in range(N):\n",
    "        g = g + ((-1/N)*y[n]*X[n,:])*logistic(-y[n]*np.dot(w,X[n,:]))\n",
    "    return g\n",
    "def grad_desc(w, X, y, lr=8e-3, converge_change=1e-3, lr_change = 1.4):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    #setup cost iter\n",
    "    E_in = [] \n",
    "    E_t = logistic_insample(X, y, w)\n",
    "    E_in.append(E_t) \n",
    "    change_E = 1 \n",
    "    t = 1 \n",
    "    convergence = 0\n",
    "    while (change_E > converge_change): #converges when there is no change in (E - E_t)\n",
    "        E = E_t\n",
    "        w = w - (lr * logistic_gradient(X,y,w))\n",
    "        #w = w - (lr*g)\n",
    "        E_t = logistic_insample(X, y, w) \n",
    "        E_in.append(E_t) #append E_t to E_in\n",
    "        change_E = E - E_t\n",
    "        \n",
    "         #E - E_t\n",
    "        #should consider changing learning rate here so if E - E_t > 0 then increase the learning rate\n",
    "        #otherwise slow it down\n",
    "        if E > E_t:\n",
    "            lr *=lr_change\n",
    "        else:\n",
    "            lr*=0.9\n",
    "        #if change_cost < converge_change:\n",
    "            #convergence = 1\n",
    "        #if np.linalg.norm(g) < 0:\n",
    "            #convergence = 1\n",
    "        \"\"\"elif i > max_i:\n",
    "            convergence = 1\"\"\"\n",
    "        t +=1\n",
    "        #t = t + 1\n",
    "    print lr, converge_change\n",
    "    return w, np.array(E_in)\n",
    "    print w.shape\n",
    "def pred_values(w, X, hard=True):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    pred_values = []\n",
    "    N, _ = X.shape\n",
    "    for n in range(N):\n",
    "        pred_prob = logistic(w.T.dot(X[n,:]) )\n",
    "        pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "        if hard:\n",
    "            pred_values.append(pred_value)\n",
    "    return pred_values\n",
    "\n",
    "r, d = data_matrix.shape\n",
    "\n",
    "betas = np.zeros(d)\n",
    "\n",
    "fitted_values, E_in = grad_desc(betas, data_matrix, y, lr=1, converge_change=1e-1, lr_change=1.1)\n",
    "print fitted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.3902137   1.29587263  2.22260581]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having working functions for logistic log likelihood and logistic gradient, let's implement gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0 0.01\n",
      "[-0.1888299  -0.00231859]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "np.random.seed(0)\n",
    "from sklearn import datasets\n",
    "\n",
    "def logistic(input):\n",
    "    out = np.exp(input)/(1+ np.exp(input))\n",
    "    return out\n",
    "def logistic_insample(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    E = 0\n",
    "    for n in range(N):\n",
    "        E = E + (1/N)*np.log(1/logistic(y[n]*np.dot(w,X[n,:])))\n",
    "    return E\n",
    "def logistic_gradient(X, y, w):\n",
    "    N, _ = X.shape\n",
    "    g = 0*w\n",
    "\n",
    "    for n in range(N):\n",
    "        g = g + ((-1/N)*y[n]*X[n,:])*logistic(-y[n]*np.dot(w,X[n,:]))\n",
    "    return g\n",
    "def grad_desc(w, X, y, lr=8e-7, converge_change=1e-10, lr_change = 1.4):\n",
    "    #normalize\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        X = np.true_divide((X - np.mean(X, axis=0)),np.std(X, axis=0))\n",
    "        X[X == np.inf] = 0\n",
    "        X = np.nan_to_num(X)\n",
    "    #setup cost iter\n",
    "    E_in = [] \n",
    "    E_t = logistic_insample(X, y, w)\n",
    "    E_in.append(E_t) \n",
    "    change_E = 1 \n",
    "    t = 1 \n",
    "    convergence = 0\n",
    "    while (change_E > converge_change): #converges when there is no change in (E - E_t)\n",
    "        E = E_t\n",
    "        w = w - (lr * logistic_gradient(X,y,w))\n",
    "        #w = w - (lr*g)\n",
    "        E_t = logistic_insample(X, y, w) \n",
    "        E_in.append(E_t) #append E_t to E_in\n",
    "        change_E = E - E_t\n",
    "        \n",
    "         #E - E_t\n",
    "        #should consider changing learning rate here so if E - E_t > 0 then increase the learning rate\n",
    "        #otherwise slow it down\n",
    "        if E > E_t:\n",
    "            lr *=lr_change\n",
    "        else:\n",
    "            lr*=0.9\n",
    "        #if change_cost < converge_change:\n",
    "            #convergence = 1\n",
    "    \n",
    "        #if np.linalg.norm(g) < 0:\n",
    "            #convergence = 1\n",
    "        \"\"\"elif i > max_i:\n",
    "            convergence = 1\"\"\"\n",
    "        t +=1\n",
    "        #t = t + 1\n",
    "    print lr, converge_change\n",
    "    return w, np.array(E_in)\n",
    "    print w.shape\n",
    "\n",
    "\n",
    "#for i in range (0,10):\n",
    "    #w, _ = grad_desc(theta_values, X, y, lr=i*1e-7, converge_change=1e-10, max_i = 1e5)\n",
    "    #pred_label = \n",
    "#This code is based http://stackoverflow.com/questions/26248654/numpy-return-0-with-divide-by-zero[1]\n",
    "\n",
    "r, d = data_matrix.shape\n",
    "\n",
    "betas = np.zeros(d)\n",
    "\n",
    "fitted_values, E_in = grad_desc(betas, data_matrix, y, lr=20, converge_change=1e-2, lr_change=0.1)\n",
    "print fitted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11733929  0.20826236]]\n",
      "(400, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(data_matrix, y)\n",
    "w = logreg.coef_\n",
    "print w\n",
    "print data_matrix.shape\n",
    "#[ 0.11733929  0.20826236]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Problem: Can you make the code faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the classification accuracy on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on? Let's try with a weight matrix $w = [-4.9494, 0.7547, 0.2691]$ obtained by waiting much longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My classification accuracy is not great -- is this just not working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(input):  \n",
    "    out = np.exp(input)/(1+np.exp(input))\n",
    "    return out\n",
    "def logistic_insample(X, y, w):\n",
    "    N, num_feat = X.shape\n",
    "    E = 0 #sum in the loop\n",
    "    for n in range(N):\n",
    "        E = E + (1/N)*np.log(1/logistic(y[n]*np.dot(w,X[n,:]))) #feeding it vectors\n",
    "    return E\n",
    "def logistic_gradient(X, y, w):\n",
    "    N, _ = X.shape\n",
    "    g = 0*w\n",
    "    \n",
    "    for n in range(N):\n",
    "        g = g + ((-1/N)*y[n]*X[n,:])*logistic(-y[n]*np.dot(w,X[n,:]))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_pred(Xorig, w):\n",
    "    #X is a dxN data matrix of input variables\n",
    "    num_pts,num_feat =Xorig.shape\n",
    "    onevec=np.ones((num_pts,1))\n",
    "    X=np.concatenate((onevec,Xorig),axis=1)\n",
    "    N, _ =X.shape\n",
    "    P=np.zeros(N)\n",
    "    for n in range(N):\n",
    "        arg=np.exp(np.dot(w,X[n,:]))\n",
    "        prob_i=arg/(1+arg)\n",
    "        P[n]=prob_i\n",
    "        \n",
    "    Pthresh=np.round(P)   #0/1 class labels\n",
    "    pred_classes=(Pthresh-0.5)*2\n",
    "    return P, pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set direction to move and take a step       \n",
    "        \n",
    "        ##############################################################\n",
    "        g=np.array(g)\n",
    "        w_new = w-learningrate*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.\n",
      "  0.  1.  0.  1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  1.  1.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.\n",
      "  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  0.  0.  1.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  1.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  1.  1.  0.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.  1.  0.\n",
      "  1.  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  1.  0.  0.  1.  0.\n",
      "  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.\n",
      "  0.  0.  0.  0.]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "y = (admission_labels-0.5)*2\n",
    "print admission_labels\n",
    "print pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
